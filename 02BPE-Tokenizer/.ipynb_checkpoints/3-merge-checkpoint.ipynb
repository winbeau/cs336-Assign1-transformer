{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652ffe5f-4e52-4920-acfe-eb597ece36fb",
   "metadata": {},
   "source": [
    "## pair counting - merge\n",
    "<input type=\"checkbox\" unchecked> ç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡(Pair counting) åœ¨\"è¯å½¢ç‰‡æ®µ\"å†…éƒ¨æŠŠç›¸é‚»å­—èŠ‚ä¸¤ä¸¤åŒ¹é…ã€è®¡ç®—å‡ºç°é¢‘ç‡\n",
    "\n",
    "<input type=\"checkbox\" unchecked> åˆå¹¶(Merge) é€‰å–æœ€é«˜é¢‘(å¹¶åˆ—é€‰æœ€é«˜å­—å…¸åº) pair åˆå¹¶æˆæ–° token é‡å¤åˆ° vocab æ»¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab67af-4966-4573-bd01-fc2c34d7f741",
   "metadata": {},
   "source": [
    "### é—®é¢˜æŠ½è±¡\n",
    "ç°æœ‰ $n$ ç»„( $n = 2 \\times 10^6 $ , æ¯ç»„é•¿åº¦çº¦ $120$ ~ $150$ )å˜é•¿å­—ç¬¦ä¸² $str_i$ ($len(str_i) \\leq 20,~i \\in [0, \\, n)$)ï¼Œæ€»é•¿åº¦ $\\sum_{i = 0}^n len(str_i) \\leq 150 \\times 10 \\times 2 \\times 10^6 = 3 \\times 10^9$ï¼Œ\n",
    "\n",
    "$vocab$ ä¸º $token ~ id$ å¯¹ $Unicode ~ byte(s)$ çš„æ˜ å°„é›†åˆï¼ˆä¹Ÿå°±æ˜¯ $token$ é›†åˆï¼‰ï¼Œåˆå§‹åŒ–ä¸º $[0, 256)$ å¯¹åº”å…¶åå…­è¿›åˆ¶æ•°æ‰€ä»£è¡¨çš„ $Unicode ~ byte$ (å¯ä»¥ç†è§£ä¸º $ASCII ~ plus$) \n",
    "\n",
    "**æŒç»­æ‰§è¡Œå¦‚ä¸‹æ“ä½œï¼š**\n",
    "\n",
    "å¯¹æ¯ä¸ªå­—ç¬¦ä¸² ***æ‰€æœ‰æœ€é•¿ $token$*** è¿›è¡Œä¸¤ä¸¤ç»“åˆç»Ÿè®¡é¢‘ç‡ï¼š\n",
    "\n",
    "<aside>\n",
    "\n",
    "åˆå§‹çŠ¶æ€æ¯ä¸ªå­—ç¬¦æ˜¯ä¸€ä¸ª $token$\n",
    "\n",
    "`word` â†’ `wo` : 1  `or` : 1  `rd` : 1\n",
    "\n",
    "> ä»€ä¹ˆæ˜¯***æœ€é•¿ $token$*** ï¼Ÿå¦‚ `newest` ï¼Œ`est` å·²åˆ†é… $token ~ id$ï¼Œæˆ‘ä»¬ä»…å¯¹ `ne` `ew` `west` å¤„ç†\n",
    "> \n",
    "</aside>\n",
    "\n",
    "å°†é¢‘ç‡æœ€é«˜çš„ä¸€ä¸ª $pair$ ï¼ˆè‹¥å¹¶åˆ—åˆ™å–æœ€é«˜å­—å…¸åºï¼‰åˆ†é…æ–°çš„ $token ~ id$ \n",
    "\n",
    "æ¯”å¦‚ä¸Šè¿° `word` åˆ™æ–°åˆ†é… `279` â†’ `wo`\n",
    "\n",
    "**æˆªæ­¢çŠ¶æ€ï¼š**\n",
    "\n",
    "æœ€ç»ˆæ•´ä¸ªåºåˆ—å·²ç»åˆ†é…äº† $token ~ id$ï¼ˆå‡ ä¹ä¸å¯èƒ½ï¼‰ æˆ– $token ~ id$ çš„æ•°ç›® = $vocab\\_size$\n",
    "\n",
    "$token ~ id \\leq vocab\\_size = 10^4$\n",
    "\n",
    "---\n",
    "\n",
    "#### è¾“å…¥\n",
    "\n",
    "äºŒç»´å­—ç¬¦ä¸²æ•°ç»„\n",
    "\n",
    "#### è¾“å‡º\n",
    "\n",
    "$vocab$ : ä» $token ~ id$ åˆ° $bytes$ çš„æ˜ å°„\n",
    "\n",
    "$merges$ : äº§ç”Ÿçš„ åˆå¹¶ $pair$ï¼ˆæŒ‰åˆ›å»ºé¡ºåºæ’åˆ—ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aee2497-7079-4c08-b035-287615fa1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, regex, time, json\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a14ffdc-86fe-4d81-b660-769b8b57c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_batch_worker(args):\n",
    "    \"\"\"å…¨å±€workerå‡½æ•°ï¼šåˆ†è¯ä¸€æ‰¹æ–‡æœ¬\"\"\"\n",
    "    batch_lines, PAT = args\n",
    "    PAT_c = regex.compile(PAT)\n",
    "    stories = []\n",
    "    for line in batch_lines:\n",
    "        tokens = [m.group(0).encode(\"utf-8\") for m in PAT_c.finditer(line)]\n",
    "        if tokens:\n",
    "            stories.append(tokens)\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8d5b4c-bc39-4922-ad6a-c8ee4f562186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_pairs_batch_worker(args):\n",
    "    \"\"\"å…¨å±€workerå‡½æ•°ï¼šç»Ÿè®¡ä¸€æ‰¹storiesçš„pairs\"\"\"\n",
    "    story_batch, special_set = args\n",
    "    pairs = Counter()\n",
    "    for story in story_batch:\n",
    "        for i in range(len(story) - 1):\n",
    "            if story[i] not in special_set and story[i+1] not in special_set:\n",
    "                pairs[(story[i], story[i+1])] += 1\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04fdaea9-23f6-47e1-a3c2-d0f67c39c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_merge_batch_worker(args):\n",
    "    \"\"\"å…¨å±€workerå‡½æ•°ï¼šå¹¶è¡Œæ‰§è¡Œmerge\"\"\"\n",
    "    story_batch, a, b, ab, special_set = args\n",
    "    local_old_deltas = Counter()\n",
    "    local_new_deltas = Counter()\n",
    "    \n",
    "    for story in story_batch:\n",
    "        i = 0\n",
    "        while i < len(story) - 1:\n",
    "            if story[i] == a and story[i+1] == b:\n",
    "                # è®°å½•æ—§pairs\n",
    "                if i > 0 and story[i-1] not in special_set:\n",
    "                    local_old_deltas[(story[i-1], a)] += 1\n",
    "                if i + 2 < len(story) and story[i+2] not in special_set:\n",
    "                    local_old_deltas[(b, story[i+2])] += 1\n",
    "                \n",
    "                # merge\n",
    "                story[i] = ab\n",
    "                story.pop(i+1)\n",
    "                \n",
    "                # è®°å½•æ–°pairs\n",
    "                if i > 0 and story[i-1] not in special_set:\n",
    "                    local_new_deltas[(story[i-1], ab)] += 1\n",
    "                if i + 1 < len(story) and story[i+1] not in special_set:\n",
    "                    local_new_deltas[(ab, story[i+1])] += 1\n",
    "                \n",
    "                i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    return story_batch, local_old_deltas, local_new_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f623e14-c788-4042-a499-2dbb0b580cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str],\n",
    "    PAT: str,\n",
    "    output_dir: str = \"./bpe_model\",\n",
    "    accurate_ratio: float = 0.6,\n",
    "    num_proc: int = None,\n",
    "    recount_interval: int = 100,\n",
    "    progress_interval: int = 100  # æ¯Næ¬¡mergeæ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    æ¸…çˆ½ç‰ˆBPEè®­ç»ƒ - é¿å…è¿›åº¦æ¡å†²çª\n",
    "    \n",
    "    ä¼˜åŒ–ï¼š\n",
    "    1. ä½¿ç”¨ç®€å•æ‰“å°ä»£æ›¿tqdmï¼ˆé¿å…å¤šè¿›ç¨‹å†²çªï¼‰\n",
    "    2. å¹¶è¡ŒåŒ–mergeæ“ä½œ\n",
    "    3. æ¸…æ™°çš„é˜¶æ®µæ˜¾ç¤º\n",
    "    \n",
    "    èµ„æºé¢„ä¼°ï¼ˆ2.2M storiesï¼‰ï¼š\n",
    "    - å†…å­˜: 15-25GB\n",
    "    - æ—¶é—´: 20-40åˆ†é’Ÿ\n",
    "    - è´¨é‡: 95%+\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if num_proc is None:\n",
    "        num_proc = max(1, cpu_count() - 2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸš€ Optimized Hybrid BPE Training\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Accurate updates: {accurate_ratio*100:.0f}% of merges\")\n",
    "    print(f\"  Workers: {num_proc}\")\n",
    "    print(f\"  Recount interval: {recount_interval}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 1ï¸âƒ£ åˆå§‹åŒ– vocab\n",
    "    # --------------------------\n",
    "    vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}\n",
    "    idx = 256\n",
    "    for tok in special_tokens:\n",
    "        vocab[idx] = tok.encode(\"utf-8\")\n",
    "        idx += 1\n",
    "    \n",
    "    special_set = set(t.encode(\"utf-8\") for t in special_tokens)\n",
    "    \n",
    "    # --------------------------\n",
    "    # 2ï¸âƒ£ å¹¶è¡ŒåŠ è½½å’Œåˆ†è¯\n",
    "    # --------------------------\n",
    "    print(\"[Step 1/3] Loading and tokenizing...\")\n",
    "    step_start = time.time()\n",
    "    \n",
    "    with open(input_path, \"rb\") as f:\n",
    "        text = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    \n",
    "    for tok in special_tokens:\n",
    "        text = text.replace(tok, f\" {tok} \")\n",
    "    \n",
    "    lines = [line for line in text.split(\"\\n\") if line.strip()]\n",
    "    \n",
    "    # æ–¹æ¡ˆ1ï¼šå‘ä¸Šå–æ•´ï¼Œç¡®ä¿æœ€å¤šnum_procä¸ªbatches\n",
    "    batch_size = (len(lines) + num_proc - 1) // num_proc  # ç­‰ä»·äº math.ceil(len(lines) / num_proc)\n",
    "    batches = [lines[i:i+batch_size] for i in range(0, len(lines), batch_size)]\n",
    "    batch_args = [(batch, PAT) for batch in batches]\n",
    "    \n",
    "    print(f\"  Tokenizing {len(lines):,} lines with {len(batches)} workers (â‰¤{num_proc})...\")\n",
    "    \n",
    "    with Pool(num_proc) as pool:\n",
    "        story_batches = pool.map(_tokenize_batch_worker, batch_args)\n",
    "    \n",
    "    all_stories = [story for batch in story_batches for story in batch]\n",
    "    total_tokens = sum(len(story) for story in all_stories)\n",
    "    \n",
    "    step_time = time.time() - step_start\n",
    "    print(f\"  âœ“ Loaded {len(all_stories):,} stories, {total_tokens:,} tokens\")\n",
    "    print(f\"  âœ“ Time: {step_time:.1f}s\\n\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 3ï¸âƒ£ å¹¶è¡Œç»Ÿè®¡åˆå§‹pairs\n",
    "    # --------------------------\n",
    "    print(\"[Step 2/3] Counting initial pairs...\")\n",
    "    step_start = time.time()\n",
    "    \n",
    "    stories_per_worker = max(1, len(all_stories) // num_proc)\n",
    "    story_batches = [\n",
    "        all_stories[i:i+stories_per_worker] \n",
    "        for i in range(0, len(all_stories), stories_per_worker)\n",
    "    ]\n",
    "    count_args = [(batch, special_set) for batch in story_batches]\n",
    "    \n",
    "    with Pool(num_proc) as pool:\n",
    "        partial_counts = pool.map(_count_pairs_batch_worker, count_args)\n",
    "    \n",
    "    pair_counts = Counter()\n",
    "    for pc in partial_counts:\n",
    "        pair_counts.update(pc)\n",
    "    \n",
    "    step_time = time.time() - step_start\n",
    "    print(f\"  âœ“ Found {len(pair_counts):,} unique pairs\")\n",
    "    print(f\"  âœ“ Time: {step_time:.1f}s\\n\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 4ï¸âƒ£ æ··åˆç­–ç•¥mergeï¼ˆå¹¶è¡ŒåŒ–ï¼‰\n",
    "    # --------------------------\n",
    "    merges: List[Tuple[bytes, bytes]] = []\n",
    "    num_merges = vocab_size - len(vocab)\n",
    "    accurate_merges = int(num_merges * accurate_ratio)\n",
    "    \n",
    "    print(f\"[Step 3/3] Performing {num_merges:,} merges\")\n",
    "    print(f\"  Phase 1: {accurate_merges:,} accurate merges (parallel)\")\n",
    "    print(f\"  Phase 2: {num_merges - accurate_merges:,} approximate merges\")\n",
    "    print(f\"  Progress will be shown every {progress_interval} merges\\n\")\n",
    "    \n",
    "    phase_start = time.time()\n",
    "    \n",
    "    # é¢„å…ˆåˆ†é…story batches\n",
    "    stories_per_worker = max(1, len(all_stories) // num_proc)\n",
    "    story_batches_fixed = [\n",
    "        all_stories[i:i+stories_per_worker] \n",
    "        for i in range(0, len(all_stories), stories_per_worker)\n",
    "    ]\n",
    "    \n",
    "    for merge_iter in range(num_merges):\n",
    "        if not pair_counts:\n",
    "            print(f\"  âš  No more pairs to merge at iteration {merge_iter}\")\n",
    "            break\n",
    "        \n",
    "        merge_start = time.time()  # è®¡æ—¶å•æ¬¡merge\n",
    "        \n",
    "        best_pair = max(pair_counts, key=pair_counts.get)\n",
    "        best_count = pair_counts[best_pair]  # è®°å½•é¢‘ç‡\n",
    "        a, b = best_pair\n",
    "        ab = a + b\n",
    "        \n",
    "        vocab[idx] = ab\n",
    "        merges.append(best_pair)\n",
    "        idx += 1\n",
    "        \n",
    "        use_accurate = merge_iter < accurate_merges\n",
    "        \n",
    "        if use_accurate:\n",
    "            # ===== å¹¶è¡Œå‡†ç¡®æ›´æ–° =====\n",
    "            merge_args = [(batch, a, b, ab, special_set) for batch in story_batches_fixed]\n",
    "            \n",
    "            with Pool(num_proc) as pool:\n",
    "                results = pool.map(_apply_merge_batch_worker, merge_args)\n",
    "            \n",
    "            # æ›´æ–°story batches\n",
    "            for i, (updated_batch, _, _) in enumerate(results):\n",
    "                story_batches_fixed[i] = updated_batch\n",
    "            \n",
    "            all_stories = [story for batch in story_batches_fixed for story in batch]\n",
    "            \n",
    "            # åˆå¹¶deltas\n",
    "            total_old_deltas = Counter()\n",
    "            total_new_deltas = Counter()\n",
    "            for _, old_deltas, new_deltas in results:\n",
    "                total_old_deltas.update(old_deltas)\n",
    "                total_new_deltas.update(new_deltas)\n",
    "            \n",
    "            # æ›´æ–°pair_counts\n",
    "            pair_counts[best_pair] = 0\n",
    "            \n",
    "            for pair, delta in total_old_deltas.items():\n",
    "                pair_counts[pair] -= delta\n",
    "                if pair_counts[pair] <= 0:\n",
    "                    del pair_counts[pair]\n",
    "            \n",
    "            for pair, delta in total_new_deltas.items():\n",
    "                pair_counts[pair] += delta\n",
    "        \n",
    "        else:\n",
    "            # ===== è¿‘ä¼¼æ›´æ–° =====\n",
    "            pair_counts.pop(best_pair, None)\n",
    "            \n",
    "            if (merge_iter - accurate_merges) % recount_interval == 0:\n",
    "                recount_start = time.time()\n",
    "                \n",
    "                count_args = [(batch, special_set) for batch in story_batches_fixed]\n",
    "                with Pool(num_proc) as pool:\n",
    "                    partial_counts = pool.map(_count_pairs_batch_worker, count_args)\n",
    "                \n",
    "                pair_counts = Counter()\n",
    "                for pc in partial_counts:\n",
    "                    pair_counts.update(pc)\n",
    "                \n",
    "                recount_time = time.time() - recount_start\n",
    "                print(f\"  [Recount at merge {merge_iter}] Time: {recount_time:.1f}s\")\n",
    "        \n",
    "        merge_time = time.time() - merge_start\n",
    "        \n",
    "        # å®šæœŸæ˜¾ç¤ºè¿›åº¦ + å®æ—¶æ€§èƒ½ç›‘æ§\n",
    "        if (merge_iter + 1) % progress_interval == 0 or merge_iter < 10:  # å‰10æ¬¡å¿…æ˜¾ç¤º\n",
    "            elapsed = time.time() - phase_start\n",
    "            rate = (merge_iter + 1) / elapsed\n",
    "            remaining = (num_merges - merge_iter - 1) / rate if rate > 0 else 0\n",
    "            percentage = (merge_iter + 1) / num_merges * 100\n",
    "            \n",
    "            phase_name = \"Accurate\" if use_accurate else \"Approximate\"\n",
    "            print(f\"  [{phase_name}] {merge_iter+1}/{num_merges} ({percentage:.1f}%) | \"\n",
    "                  f\"Last merge: {merge_time:.1f}s (freq={best_count:,}) | \"\n",
    "                  f\"Avg rate: {rate:.2f}/s | ETA: {remaining/60:.1f}min\")\n",
    "        \n",
    "        # é˜¶æ®µåˆ‡æ¢æç¤º\n",
    "        if merge_iter == accurate_merges - 1:\n",
    "            phase_time = time.time() - phase_start\n",
    "            print(f\"\\n  âœ“ Accurate phase complete ({phase_time/60:.1f}min)\")\n",
    "            print(f\"  â†’ Switching to approximate mode (faster)...\\n\")\n",
    "            phase_start = time.time()\n",
    "    \n",
    "    total_merge_time = time.time() - (step_start - step_time)\n",
    "    print(f\"\\n  âœ“ All merges complete\")\n",
    "    print(f\"  âœ“ Time: {total_merge_time/60:.1f}min\\n\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 5ï¸âƒ£ ä¿å­˜æ¨¡å‹\n",
    "    # --------------------------\n",
    "    print(\"[Step 4/4] Saving model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    vocab_path = os.path.join(output_dir, \"vocab.json\")\n",
    "    merges_path = os.path.join(output_dir, \"merges.txt\")\n",
    "    \n",
    "    vocab_serialized = {str(k): v.decode(\"utf-8\", errors=\"replace\") for k, v in vocab.items()}\n",
    "    with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_serialized, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(merges_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b in merges:\n",
    "            f.write(f\"{a.decode('utf-8', errors='replace')} {b.decode('utf-8', errors='replace')}\\n\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  âœ“ Saved to {output_dir}\\n\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"âœ… BPE training completed!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Total time: {elapsed/60:.1f} minutes ({elapsed:.0f}s)\")\n",
    "    print(f\"  Final vocab size: {len(vocab)}\")\n",
    "    print(f\"  Total merges: {len(merges)}\")\n",
    "    print(f\"  Average rate: {len(merges)/elapsed:.2f} merges/s\")\n",
    "    print(f\"  Output directory: {output_dir}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d250fa9-7f90-49f5-8512-2503a20b55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/winbeau/Study/1-transformer/datasets/TinyStories/txt/valid_with_eot.txt\"\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "vocab, merges = train_bpe(\n",
    "    input_path=train_path,\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    PAT=PAT,\n",
    "    output_dir=\"./bpe_model_hybrid\",\n",
    "    accurate_ratio=0.6,  # å‰60%ç²¾ç¡®ï¼Œå40%è¿‘ä¼¼\n",
    "    num_proc=12, \n",
    "    recount_interval=100, # æ§åˆ¶æœ€åè¿‘ä¼¼æ›´æ–°çš„é‡æ–°ç»Ÿè®¡æ›´æ–°é¢‘ç‡ | é»˜è®¤100æ¬¡\n",
    "    progress_interval=100  # æ¯100æ¬¡mergeæ˜¾ç¤ºä¸€æ¬¡ | å‰10æ¬¡å¿…æ˜¾ç¤º\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f7cfd-ea56-4c3c-871e-1aab8e837801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
