{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e84f8a-7342-4cb5-b3e8-738854479ec3",
   "metadata": {},
   "source": [
    "## 2-BPE | 预分词(Pre-tokenization) \\[ 多进程并行 \\]\n",
    "BPE-No.1: 预分词(Pre-tokenization) 把原始文本分成初步\"词形片段\"并计数(正则化去掉标点、符号)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91665f99-02ed-4cf3-adf5-3f1e6e034af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re \n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from collections import Counter\n",
    "from cs336_pretokenization_example import find_chunk_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c92a48-de64-4daf-97f9-f5e56df97214",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/winbeau/Study/1-transformer/datasets/TinyStories/txt/train_with_eot.txt\"\n",
    "assert os.path.exists(train_path), \"Not found train_with_eot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c70bf-56d6-482b-af70-5b2b95fe39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = min(12, cpu_count())\n",
    "print(f\"Using {num_processes} processes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af36cc2-0d8b-431c-bae8-060b3eb64436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex 正则化 减弱标点、其他符号对文本的影响\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "SPECIAL = \"<|endoftext|>\" # 结束标志特殊正则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b155f-2f2f-4eb0-99c7-eefe33ed8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(start_end):\n",
    "    start, end = start_end\n",
    "    counter = Counter()\n",
    "\n",
    "    with open(train_path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    idx = 0\n",
    "    while True: # 手动查找 <|endoftext|> 位置\n",
    "        pos = chunk.find(SPECIAL, idx)\n",
    "        if pos == -1: # 没找到 对剩下部分用正则分词\n",
    "            part = chunk[idx:]\n",
    "            for m in re.finditer(PAT, part):\n",
    "                tok = m.group()\n",
    "                counter[tok.encode(\"utf-8\")] += 1\n",
    "            break\n",
    "        # 对特殊 token 前面的部分做分词\n",
    "        part = chunk[idx:pos]\n",
    "        for m in re.finditer(PAT, part):\n",
    "            tok = m.group()\n",
    "            counter[tok.encode(\"utf-8\")] += 1\n",
    "        # 单独计一次特殊 token\n",
    "        counter[SPECIAL.encode(\"utf-8\")] += 1\n",
    "        idx = pos + len(SPECIAL)\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1685399-bd19-4b62-b67a-bc36342cc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path, \"rb\") as f: \n",
    "    boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "chunk_pairs = list(zip(boundaries[:-1], boundaries[1:])) # 0 1 -> 1 2 \n",
    "print(f\"Found {len(chunk_pairs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3fdd8-e168-48e4-ade9-b6ce92524590",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(num_processes) as p: \n",
    "    counters = list(tqdm(\n",
    "        p.imap(process_chunk, chunk_pairs), # 若不想加过程可视化模块直接 p.imap 即可\n",
    "        total=len(chunk_pairs), \n",
    "        desc=\"Pre-tokenization chunks\", \n",
    "        ncols=80\n",
    "    ))\n",
    "\n",
    "total_counts = sum(counters, Counter())\n",
    "print(f\"Total unique tokens: {len(total_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ef0a2-485d-4067-8b57-398cae16e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 most common tokens:\") # 可以看到有很多前导空格————' '频率极高，使用前导空格优化效率\n",
    "for token, freq in total_counts.most_common(10): \n",
    "    try: \n",
    "        print(f\"{token.decode('utf-8', errors='ignore')!r} : {freq} \")\n",
    "    except Exception: \n",
    "        print(f\"{token} : {freq}\")\n",
    "print(f\"<|endoftext|> freq: {total_counts.get(b'<|endoftext|>', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccec8c-e54a-4e2a-bf21-bc15f433973a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
