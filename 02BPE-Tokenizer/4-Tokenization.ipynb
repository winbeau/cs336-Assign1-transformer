{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec721638-7428-4be1-a3a8-e5c50003cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, regex\n",
    "from typing import Dict, List, Tuple, Iterable, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c89efc-f650-4d2f-9a7a-68a9b5c009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Dict[int, bytes],\n",
    "        merges: List[Tuple[bytes, bytes]],\n",
    "        special_tokens: List[str] | None = None,\n",
    "    ):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_rev = {v: k for k, v in vocab.items()}\n",
    "        self.special_tokens = special_tokens or []\n",
    "\n",
    "        # ç¼–è¯‘æ­£åˆ™ï¼Œä¸è®­ç»ƒä¸€è‡´ï¼ˆå‰å¯¼ç©ºæ ¼ï¼‰\n",
    "        self.PAT = regex.compile(\n",
    "            r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        )\n",
    "\n",
    "        # é¢„å»º merge rank è¡¨\n",
    "        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    @classmethod\n",
    "    def from_files(\n",
    "        cls,\n",
    "        vocab_path: str,\n",
    "        merges_path: str,\n",
    "        special_tokens: List[str] | None = None,\n",
    "    ) -> \"Tokenizer\":\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab_data = json.load(f)\n",
    "        vocab = {int(i): v.encode(\"utf-8\") for i, v in vocab_data.items()}\n",
    "\n",
    "        merges = []\n",
    "        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # å¿½ç•¥æ³¨é‡Šå’Œéæ³•è¡Œ\n",
    "                if not line or line.startswith(\"#\"):\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    continue  # è·³è¿‡ä¸åˆè§„çš„è¡Œ\n",
    "                a, b = parts\n",
    "                merges.append((a.encode(), b.encode()))\n",
    "\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def get_pairs(self, tokens: List[bytes]):\n",
    "        \"\"\"å–ç›¸é‚» token pair\"\"\"\n",
    "        return {(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)}\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def bpe(self, token: bytes) -> List[bytes]:\n",
    "        \"\"\"æ‰§è¡Œ Byte-Pair Encoding\"\"\"\n",
    "        word = [bytes([b]) for b in token]  # byte-level åˆå§‹åˆ†è¯\n",
    "        pairs = self.get_pairs(word)\n",
    "\n",
    "        while pairs:\n",
    "            # æ‰¾åˆ°å½“å‰å¯åˆå¹¶çš„æœ€å° rank\n",
    "            min_pair = min(\n",
    "                pairs, key=lambda p: self.bpe_ranks.get(p, float(\"inf\"))\n",
    "            )\n",
    "            if min_pair not in self.bpe_ranks:\n",
    "                break\n",
    "\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if (\n",
    "                    i < len(word) - 1\n",
    "                    and word[i] == min_pair[0]\n",
    "                    and word[i + 1] == min_pair[1]\n",
    "                ):\n",
    "                    new_word.append(word[i] + word[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            word = new_word\n",
    "            pairs = self.get_pairs(word)\n",
    "\n",
    "        return word\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text â†’ token ids\"\"\"\n",
    "        ids = []\n",
    "        for match in self.PAT.finditer(text):\n",
    "            token = match.group(0).encode(\"utf-8\")\n",
    "            for t in self.bpe(token):\n",
    "                if t in self.vocab_rev:\n",
    "                    ids.append(self.vocab_rev[t])\n",
    "        return ids\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for line in iterable:\n",
    "            for tid in self.encode(line):\n",
    "                yield tid\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        byte_stream = b\"\".join(\n",
    "            self.vocab.get(i, b\"\\xef\\xbf\\xbd\") for i in ids\n",
    "        )\n",
    "        return byte_stream.decode(\"utf-8\", errors=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f73f53c0-c8fa-493b-a1f5-752e13febb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33, 32, 73, 116, 39, 115, 32, 97, 32, 116, 101, 115, 116, 46]\n",
      "Hello world! It's a test.\n"
     ]
    }
   ],
   "source": [
    "toks = Tokenizer.from_files(\"./bpe_model_hybrid/vocab.json\", \"./bpe_model_hybrid/merges.txt\")\n",
    "\n",
    "sample = \"Hello world! It's a test.\"\n",
    "ids = toks.encode(sample)\n",
    "print(ids)\n",
    "print(toks.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64197b4a-508a-4efe-84a2-6007ecf500c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# from tokenizer import Tokenizer  # ä½ åœ¨ 4-Tokenization.ipynb é‡Œå®ç°çš„ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bf13db-73cf-4576-b06f-1e90d773dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer.from_files(\n",
    "    \"./bpe_model_hybrid/vocab.json\",\n",
    "    \"./bpe_model_hybrid/merges.txt\",\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cd2983f-8886-4532-ac29-78a5ae64171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408133it [05:07, 1327.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m         ids\u001b[38;5;241m.\u001b[39mextend(\u001b[43mtok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# è½¬ numpy æ•°ç»„\u001b[39;00m\n\u001b[1;32m     13\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ids, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint16)\n",
      "Cell \u001b[0;32mIn[2], line 90\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     88\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPAT\u001b[38;5;241m.\u001b[39mfinditer(text):\n\u001b[0;32m---> 90\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbpe(token):\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_rev:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_txt_path = \"../datasets/TinyStories/txt/train_with_eot.txt\"\n",
    "output_tokens_path = \"../datasets/TinyStories/tokens_train.npy\"\n",
    "\n",
    "ids = []\n",
    "with open(input_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        ids.extend(tok.encode(line))\n",
    "\n",
    "# è½¬ numpy æ•°ç»„\n",
    "arr = np.array(ids, dtype=np.uint16)\n",
    "np.save(output_tokens_path, arr)\n",
    "print(f\"Saved tokenized train set to {output_tokens_path}, total {len(arr)} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8154ed0b-60a6-4fd8-851d-8b04d8bcdfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "# from tokenizer import Tokenizer  # ä½ è‡ªå·±çš„ Tokenizer ç±»\n",
    "\n",
    "# === åˆå§‹åŒ–å…¨å±€å˜é‡ï¼ˆé¿å…é‡å¤åŠ è½½ vocab/mergesï¼‰ ===\n",
    "TOKENIZER = None\n",
    "\n",
    "def _init_worker(vocab_path, merges_path, special_tokens):\n",
    "    global TOKENIZER\n",
    "    TOKENIZER = Tokenizer.from_files(vocab_path, merges_path, special_tokens=special_tokens)\n",
    "\n",
    "def encode_line(line: str):\n",
    "    global TOKENIZER\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return []\n",
    "    return TOKENIZER.encode(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b25b7e-3119-41e0-9ba1-9b364c5d06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_encode_file(\n",
    "    input_txt_path: str,\n",
    "    output_tokens_path: str,\n",
    "    vocab_path: str = \"./bpe_model_hybrid/vocab.json\",\n",
    "    merges_path: str = \"./bpe_model_hybrid/merges.txt\",\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    num_workers: int = max(1, cpu_count() - 4),\n",
    "):\n",
    "    print(f\"ğŸ”§ Using {num_workers} processes to encode {input_txt_path}\")\n",
    "\n",
    "    # 1. è¯»å–æ‰€æœ‰è¡Œ\n",
    "    with open(input_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line for line in f if line.strip()]\n",
    "\n",
    "    # 2. å¤šè¿›ç¨‹å¹¶è¡Œ encode\n",
    "    with Pool(\n",
    "        processes=num_workers,\n",
    "        initializer=_init_worker,\n",
    "        initargs=(vocab_path, merges_path, special_tokens),\n",
    "    ) as pool:\n",
    "        encoded = list(tqdm(pool.imap(encode_line, lines, chunksize=128), total=len(lines)))\n",
    "\n",
    "    # 3. æ‹¼æ¥æ‰€æœ‰ ids å¹¶ä¿å­˜\n",
    "    all_ids = np.fromiter((i for seq in encoded for i in seq), dtype=np.uint16)\n",
    "    np.save(output_tokens_path, all_ids)\n",
    "    print(f\"âœ… Saved {len(all_ids):,} tokens â†’ {output_tokens_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b939c-11f1-4645-83fd-d711721a4451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Using 12 processes to encode ../datasets/TinyStories/txt/train_with_eot.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                        | 1398913/2119489 [06:03<02:41, 4468.57it/s]"
     ]
    }
   ],
   "source": [
    "parallel_encode_file(\n",
    "    input_txt_path=\"../datasets/TinyStories/txt/train_with_eot.txt\",\n",
    "    output_tokens_path=\"../datasets/TinyStories/tokens_train.npy\",\n",
    ")\n",
    "\n",
    "# parallel_encode_file(\n",
    "#     input_txt_path=\"../datasets/TinyStories/txt/valid_with_eot.txt\",\n",
    "#     output_tokens_path=\"../datasets/TinyStories/tokens_valid.npy\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065fb414-283a-4ae2-8050-028d8dff060a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2541a-09d6-4575-921b-2a4d0f369a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CS336)",
   "language": "python",
   "name": "cs336"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
