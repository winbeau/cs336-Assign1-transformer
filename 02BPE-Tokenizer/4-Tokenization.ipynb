{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec721638-7428-4be1-a3a8-e5c50003cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, regex\n",
    "from typing import Dict, List, Tuple, Iterable, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4c89efc-f650-4d2f-9a7a-68a9b5c009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Dict[int, bytes],\n",
    "        merges: List[Tuple[bytes, bytes]],\n",
    "        special_tokens: List[str] | None = None,\n",
    "    ):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_rev = {v: k for k, v in vocab.items()}\n",
    "        self.special_tokens = special_tokens or []\n",
    "\n",
    "        # 编译正则，与训练一致（前导空格）\n",
    "        self.PAT = regex.compile(\n",
    "            r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        )\n",
    "\n",
    "        # 预建 merge rank 表\n",
    "        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    @classmethod\n",
    "    def from_files(\n",
    "        cls,\n",
    "        vocab_path: str,\n",
    "        merges_path: str,\n",
    "        special_tokens: List[str] | None = None,\n",
    "    ) -> \"Tokenizer\":\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab_data = json.load(f)\n",
    "        vocab = {int(i): v.encode(\"utf-8\") for i, v in vocab_data.items()}\n",
    "\n",
    "        merges = []\n",
    "        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # 忽略注释和非法行\n",
    "                if not line or line.startswith(\"#\"):\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    continue  # 跳过不合规的行\n",
    "                a, b = parts\n",
    "                merges.append((a.encode(), b.encode()))\n",
    "\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def get_pairs(self, tokens: List[bytes]):\n",
    "        \"\"\"取相邻 token pair\"\"\"\n",
    "        return {(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)}\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def bpe(self, token: bytes) -> List[bytes]:\n",
    "        \"\"\"执行 Byte-Pair Encoding\"\"\"\n",
    "        word = [bytes([b]) for b in token]  # byte-level 初始分词\n",
    "        pairs = self.get_pairs(word)\n",
    "\n",
    "        while pairs:\n",
    "            # 找到当前可合并的最小 rank\n",
    "            min_pair = min(\n",
    "                pairs, key=lambda p: self.bpe_ranks.get(p, float(\"inf\"))\n",
    "            )\n",
    "            if min_pair not in self.bpe_ranks:\n",
    "                break\n",
    "\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if (\n",
    "                    i < len(word) - 1\n",
    "                    and word[i] == min_pair[0]\n",
    "                    and word[i + 1] == min_pair[1]\n",
    "                ):\n",
    "                    new_word.append(word[i] + word[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            word = new_word\n",
    "            pairs = self.get_pairs(word)\n",
    "\n",
    "        return word\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text → token ids\"\"\"\n",
    "        ids = []\n",
    "        for match in self.PAT.finditer(text):\n",
    "            token = match.group(0).encode(\"utf-8\")\n",
    "            for t in self.bpe(token):\n",
    "                if t in self.vocab_rev:\n",
    "                    ids.append(self.vocab_rev[t])\n",
    "        return ids\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for line in iterable:\n",
    "            for tid in self.encode(line):\n",
    "                yield tid\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        byte_stream = b\"\".join(\n",
    "            self.vocab.get(i, b\"\\xef\\xbf\\xbd\") for i in ids\n",
    "        )\n",
    "        return byte_stream.decode(\"utf-8\", errors=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f73f53c0-c8fa-493b-a1f5-752e13febb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33, 32, 73, 116, 39, 115, 32, 97, 32, 116, 101, 115, 116, 46]\n",
      "Hello world! It's a test.\n"
     ]
    }
   ],
   "source": [
    "toks = Tokenizer.from_files(\"./bpe_model_hybrid/vocab.json\", \"./bpe_model_hybrid/merges.txt\")\n",
    "\n",
    "sample = \"Hello world! It's a test.\"\n",
    "ids = toks.encode(sample)\n",
    "print(ids)\n",
    "print(toks.decode(ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
