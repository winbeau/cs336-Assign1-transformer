{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652ffe5f-4e52-4920-acfe-eb597ece36fb",
   "metadata": {},
   "source": [
    "## pair counting - merge\n",
    "<input type=\"checkbox\" unchecked> ç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡(Pair counting) åœ¨\"è¯å½¢ç‰‡æ®µ\"å†…éƒ¨æŠŠç›¸é‚»å­—èŠ‚ä¸¤ä¸¤åŒ¹é…ã€è®¡ç®—å‡ºç°é¢‘ç‡\n",
    "\n",
    "<input type=\"checkbox\" unchecked> åˆå¹¶(Merge) é€‰å–æœ€é«˜é¢‘(å¹¶åˆ—é€‰æœ€é«˜å­—å…¸åº) pair åˆå¹¶æˆæ–° token é‡å¤åˆ° vocab æ»¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab67af-4966-4573-bd01-fc2c34d7f741",
   "metadata": {},
   "source": [
    "### é—®é¢˜æŠ½è±¡\n",
    "ç°æœ‰ $n$ ç»„( $n = 2 \\times 10^6 $ , æ¯ç»„é•¿åº¦çº¦ $120$ ~ $150$ )å˜é•¿å­—ç¬¦ä¸² $str_i$ ($len(str_i) \\leq 20,~i \\in [0, \\, n)$)ï¼Œæ€»é•¿åº¦ $\\sum_{i = 0}^n len(str_i) \\leq 150 \\times 10 \\times 2 \\times 10^6 = 3 \\times 10^9$ï¼Œ\n",
    "\n",
    "$vocab$ ä¸º $token ~ id$ å¯¹ $Unicode ~ byte(s)$ çš„æ˜ å°„é›†åˆï¼ˆä¹Ÿå°±æ˜¯ $token$ é›†åˆï¼‰ï¼Œåˆå§‹åŒ–ä¸º $[0, 256)$ å¯¹åº”å…¶åå…­è¿›åˆ¶æ•°æ‰€ä»£è¡¨çš„ $Unicode ~ byte$ (å¯ä»¥ç†è§£ä¸º $ASCII ~ plus$) \n",
    "\n",
    "**æŒç»­æ‰§è¡Œå¦‚ä¸‹æ“ä½œï¼š**\n",
    "\n",
    "å¯¹æ¯ä¸ªå­—ç¬¦ä¸² ***æ‰€æœ‰æœ€é•¿ $token$*** è¿›è¡Œä¸¤ä¸¤ç»“åˆç»Ÿè®¡é¢‘ç‡ï¼š\n",
    "\n",
    "<aside>\n",
    "\n",
    "åˆå§‹çŠ¶æ€æ¯ä¸ªå­—ç¬¦æ˜¯ä¸€ä¸ª $token$\n",
    "\n",
    "`word` â†’ `wo` : 1  `or` : 1  `rd` : 1\n",
    "\n",
    "> ä»€ä¹ˆæ˜¯***æœ€é•¿ $token$*** ï¼Ÿå¦‚ `newest` ï¼Œ`est` å·²åˆ†é… $token ~ id$ï¼Œæˆ‘ä»¬ä»…å¯¹ `ne` `ew` `west` å¤„ç†\n",
    "> \n",
    "</aside>\n",
    "\n",
    "å°†é¢‘ç‡æœ€é«˜çš„ä¸€ä¸ª $pair$ ï¼ˆè‹¥å¹¶åˆ—åˆ™å–æœ€é«˜å­—å…¸åºï¼‰åˆ†é…æ–°çš„ $token ~ id$ \n",
    "\n",
    "æ¯”å¦‚ä¸Šè¿° `word` åˆ™æ–°åˆ†é… `279` â†’ `wo`\n",
    "\n",
    "**æˆªæ­¢çŠ¶æ€ï¼š**\n",
    "\n",
    "æœ€ç»ˆæ•´ä¸ªåºåˆ—å·²ç»åˆ†é…äº† $token ~ id$ï¼ˆå‡ ä¹ä¸å¯èƒ½ï¼‰ æˆ– $token ~ id$ çš„æ•°ç›® = $vocab\\_size$\n",
    "\n",
    "$token ~ id \\leq vocab\\_size = 10^4$\n",
    "\n",
    "---\n",
    "\n",
    "#### è¾“å…¥\n",
    "\n",
    "äºŒç»´å­—ç¬¦ä¸²æ•°ç»„\n",
    "\n",
    "#### è¾“å‡º\n",
    "\n",
    "$vocab$ : ä» $token ~ id$ åˆ° $bytes$ çš„æ˜ å°„\n",
    "\n",
    "$merges$ : äº§ç”Ÿçš„ åˆå¹¶ $pair$ï¼ˆæŒ‰åˆ›å»ºé¡ºåºæ’åˆ—ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aee2497-7079-4c08-b035-287615fa1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, regex, time, json\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a14ffdc-86fe-4d81-b660-769b8b57c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_batch_worker(args):\n",
    "    \"\"\"å…¨å±€workerå‡½æ•°ï¼šåˆ†è¯ä¸€æ‰¹æ–‡æœ¬\"\"\"\n",
    "    batch_lines, PAT = args\n",
    "    PAT_c = regex.compile(PAT)\n",
    "    stories = []\n",
    "    for line in batch_lines:\n",
    "        tokens = [m.group(0).encode(\"utf-8\") for m in PAT_c.finditer(line)]\n",
    "        if tokens:\n",
    "            stories.append(tokens)\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8d5b4c-bc39-4922-ad6a-c8ee4f562186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_pairs_batch_worker(args):\n",
    "    \"\"\"å…¨å±€workerå‡½æ•°ï¼šç»Ÿè®¡ä¸€æ‰¹storiesçš„pairs\"\"\"\n",
    "    story_batch, special_set = args\n",
    "    pairs = Counter()\n",
    "    for story in story_batch:\n",
    "        for i in range(len(story) - 1):\n",
    "            if story[i] not in special_set and story[i+1] not in special_set:\n",
    "                pairs[(story[i], story[i+1])] += 1\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04fdaea9-23f6-47e1-a3c2-d0f67c39c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_merge_batch_worker(args):\n",
    "    \"\"\"å…¨å±€workerå‡½æ•°ï¼šå¹¶è¡Œæ‰§è¡Œmerge\"\"\"\n",
    "    story_batch, a, b, ab, special_set = args\n",
    "    local_old_deltas = Counter()\n",
    "    local_new_deltas = Counter()\n",
    "    \n",
    "    for story in story_batch:\n",
    "        i = 0\n",
    "        while i < len(story) - 1:\n",
    "            if story[i] == a and story[i+1] == b:\n",
    "                # è®°å½•æ—§pairs\n",
    "                if i > 0 and story[i-1] not in special_set:\n",
    "                    local_old_deltas[(story[i-1], a)] += 1\n",
    "                if i + 2 < len(story) and story[i+2] not in special_set:\n",
    "                    local_old_deltas[(b, story[i+2])] += 1\n",
    "                \n",
    "                # merge\n",
    "                story[i] = ab\n",
    "                story.pop(i+1)\n",
    "                \n",
    "                # è®°å½•æ–°pairs\n",
    "                if i > 0 and story[i-1] not in special_set:\n",
    "                    local_new_deltas[(story[i-1], ab)] += 1\n",
    "                if i + 1 < len(story) and story[i+1] not in special_set:\n",
    "                    local_new_deltas[(ab, story[i+1])] += 1\n",
    "                \n",
    "                i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    return story_batch, local_old_deltas, local_new_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f623e14-c788-4042-a499-2dbb0b580cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str],\n",
    "    PAT: str,\n",
    "    output_dir: str = \"./bpe_model\",\n",
    "    accurate_ratio: float = 0.6,\n",
    "    num_proc: int = None,\n",
    "    recount_interval: int = 100,\n",
    "    progress_interval: int = 100  # æ¯Næ¬¡mergeæ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "    \"\"\"\n",
    "    æ¸…çˆ½ç‰ˆBPEè®­ç»ƒ - é¿å…è¿›åº¦æ¡å†²çª\n",
    "    \n",
    "    ä¼˜åŒ–ï¼š\n",
    "    1. ä½¿ç”¨ç®€å•æ‰“å°ä»£æ›¿tqdmï¼ˆé¿å…å¤šè¿›ç¨‹å†²çªï¼‰\n",
    "    2. å¹¶è¡ŒåŒ–mergeæ“ä½œ\n",
    "    3. æ¸…æ™°çš„é˜¶æ®µæ˜¾ç¤º\n",
    "    \n",
    "    èµ„æºé¢„ä¼°ï¼ˆ2.2M storiesï¼‰ï¼š\n",
    "    - å†…å­˜: 15-25GB\n",
    "    - æ—¶é—´: 20-40åˆ†é’Ÿ\n",
    "    - è´¨é‡: 95%+\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if num_proc is None:\n",
    "        num_proc = max(1, cpu_count() - 2)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸš€ Optimized Hybrid BPE Training\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Accurate updates: {accurate_ratio*100:.0f}% of merges\")\n",
    "    print(f\"  Workers: {num_proc}\")\n",
    "    print(f\"  Recount interval: {recount_interval}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 1ï¸âƒ£ åˆå§‹åŒ– vocab\n",
    "    # --------------------------\n",
    "    vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}\n",
    "    idx = 256\n",
    "    for tok in special_tokens:\n",
    "        vocab[idx] = tok.encode(\"utf-8\")\n",
    "        idx += 1\n",
    "    \n",
    "    special_set = set(t.encode(\"utf-8\") for t in special_tokens)\n",
    "    \n",
    "    # --------------------------\n",
    "    # 2ï¸âƒ£ å¹¶è¡ŒåŠ è½½å’Œåˆ†è¯\n",
    "    # --------------------------\n",
    "    print(\"[Step 1/3] Loading and tokenizing...\")\n",
    "    step_start = time.time()\n",
    "    \n",
    "    with open(input_path, \"rb\") as f:\n",
    "        text = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    \n",
    "    for tok in special_tokens:\n",
    "        text = text.replace(tok, f\" {tok} \")\n",
    "    \n",
    "    lines = [line for line in text.split(\"\\n\") if line.strip()]\n",
    "    \n",
    "    # æ–¹æ¡ˆ1ï¼šå‘ä¸Šå–æ•´ï¼Œç¡®ä¿æœ€å¤šnum_procä¸ªbatches\n",
    "    batch_size = (len(lines) + num_proc - 1) // num_proc  # ç­‰ä»·äº math.ceil(len(lines) / num_proc)\n",
    "    batches = [lines[i:i+batch_size] for i in range(0, len(lines), batch_size)]\n",
    "    batch_args = [(batch, PAT) for batch in batches]\n",
    "    \n",
    "    print(f\"  Tokenizing {len(lines):,} lines with {len(batches)} workers (â‰¤{num_proc})...\")\n",
    "    \n",
    "    with Pool(num_proc) as pool:\n",
    "        story_batches = pool.map(_tokenize_batch_worker, batch_args)\n",
    "    \n",
    "    all_stories = [story for batch in story_batches for story in batch]\n",
    "    total_tokens = sum(len(story) for story in all_stories)\n",
    "    \n",
    "    step_time = time.time() - step_start\n",
    "    print(f\"  âœ“ Loaded {len(all_stories):,} stories, {total_tokens:,} tokens\")\n",
    "    print(f\"  âœ“ Time: {step_time:.1f}s\\n\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 3ï¸âƒ£ å¹¶è¡Œç»Ÿè®¡åˆå§‹pairs\n",
    "    # --------------------------\n",
    "    print(\"[Step 2/3] Counting initial pairs...\")\n",
    "    step_start = time.time()\n",
    "    \n",
    "    stories_per_worker = max(1, len(all_stories) // num_proc)\n",
    "    story_batches = [\n",
    "        all_stories[i:i+stories_per_worker] \n",
    "        for i in range(0, len(all_stories), stories_per_worker)\n",
    "    ]\n",
    "    count_args = [(batch, special_set) for batch in story_batches]\n",
    "    \n",
    "    with Pool(num_proc) as pool:\n",
    "        partial_counts = pool.map(_count_pairs_batch_worker, count_args)\n",
    "    \n",
    "    pair_counts = Counter()\n",
    "    for pc in partial_counts:\n",
    "        pair_counts.update(pc)\n",
    "    \n",
    "    step_time = time.time() - step_start\n",
    "    print(f\"  âœ“ Found {len(pair_counts):,} unique pairs\")\n",
    "    print(f\"  âœ“ Time: {step_time:.1f}s\\n\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 4ï¸âƒ£ æ··åˆç­–ç•¥mergeï¼ˆå¹¶è¡ŒåŒ–ï¼‰\n",
    "    # --------------------------\n",
    "    merges: List[Tuple[bytes, bytes]] = []\n",
    "    num_merges = vocab_size - len(vocab)\n",
    "    accurate_merges = int(num_merges * accurate_ratio)\n",
    "    \n",
    "    print(f\"[Step 3/3] Performing {num_merges:,} merges\")\n",
    "    print(f\"  Phase 1: {accurate_merges:,} accurate merges (parallel)\")\n",
    "    print(f\"  Phase 2: {num_merges - accurate_merges:,} approximate merges\")\n",
    "    print(f\"  Progress will be shown every {progress_interval} merges\\n\")\n",
    "    \n",
    "    phase_start = time.time()\n",
    "    \n",
    "    # é¢„å…ˆåˆ†é…story batches\n",
    "    stories_per_worker = max(1, len(all_stories) // num_proc)\n",
    "    story_batches_fixed = [\n",
    "        all_stories[i:i+stories_per_worker] \n",
    "        for i in range(0, len(all_stories), stories_per_worker)\n",
    "    ]\n",
    "    \n",
    "    for merge_iter in range(num_merges):\n",
    "        if not pair_counts:\n",
    "            print(f\"  âš  No more pairs to merge at iteration {merge_iter}\")\n",
    "            break\n",
    "        \n",
    "        merge_start = time.time()  # è®¡æ—¶å•æ¬¡merge\n",
    "        \n",
    "        best_pair = max(pair_counts, key=pair_counts.get)\n",
    "        best_count = pair_counts[best_pair]  # è®°å½•é¢‘ç‡\n",
    "        a, b = best_pair\n",
    "        ab = a + b\n",
    "        \n",
    "        vocab[idx] = ab\n",
    "        merges.append(best_pair)\n",
    "        idx += 1\n",
    "        \n",
    "        use_accurate = merge_iter < accurate_merges\n",
    "        \n",
    "        if use_accurate:\n",
    "            # ===== å¹¶è¡Œå‡†ç¡®æ›´æ–° =====\n",
    "            merge_args = [(batch, a, b, ab, special_set) for batch in story_batches_fixed]\n",
    "            \n",
    "            with Pool(num_proc) as pool:\n",
    "                results = pool.map(_apply_merge_batch_worker, merge_args)\n",
    "            \n",
    "            # æ›´æ–°story batches\n",
    "            for i, (updated_batch, _, _) in enumerate(results):\n",
    "                story_batches_fixed[i] = updated_batch\n",
    "            \n",
    "            all_stories = [story for batch in story_batches_fixed for story in batch]\n",
    "            \n",
    "            # åˆå¹¶deltas\n",
    "            total_old_deltas = Counter()\n",
    "            total_new_deltas = Counter()\n",
    "            for _, old_deltas, new_deltas in results:\n",
    "                total_old_deltas.update(old_deltas)\n",
    "                total_new_deltas.update(new_deltas)\n",
    "            \n",
    "            # æ›´æ–°pair_counts\n",
    "            pair_counts[best_pair] = 0\n",
    "            \n",
    "            for pair, delta in total_old_deltas.items():\n",
    "                pair_counts[pair] -= delta\n",
    "                if pair_counts[pair] <= 0:\n",
    "                    del pair_counts[pair]\n",
    "            \n",
    "            for pair, delta in total_new_deltas.items():\n",
    "                pair_counts[pair] += delta\n",
    "        \n",
    "        else:\n",
    "            # ===== è¿‘ä¼¼æ›´æ–° =====\n",
    "            pair_counts.pop(best_pair, None)\n",
    "            \n",
    "            if (merge_iter - accurate_merges) % recount_interval == 0:\n",
    "                recount_start = time.time()\n",
    "                \n",
    "                count_args = [(batch, special_set) for batch in story_batches_fixed]\n",
    "                with Pool(num_proc) as pool:\n",
    "                    partial_counts = pool.map(_count_pairs_batch_worker, count_args)\n",
    "                \n",
    "                pair_counts = Counter()\n",
    "                for pc in partial_counts:\n",
    "                    pair_counts.update(pc)\n",
    "                \n",
    "                recount_time = time.time() - recount_start\n",
    "                print(f\"  [Recount at merge {merge_iter}] Time: {recount_time:.1f}s\")\n",
    "        \n",
    "        merge_time = time.time() - merge_start\n",
    "        \n",
    "        # å®šæœŸæ˜¾ç¤ºè¿›åº¦ + å®æ—¶æ€§èƒ½ç›‘æ§\n",
    "        if (merge_iter + 1) % progress_interval == 0 or merge_iter < 10:  # å‰10æ¬¡å¿…æ˜¾ç¤º\n",
    "            elapsed = time.time() - phase_start\n",
    "            rate = (merge_iter + 1) / elapsed\n",
    "            remaining = (num_merges - merge_iter - 1) / rate if rate > 0 else 0\n",
    "            percentage = (merge_iter + 1) / num_merges * 100\n",
    "            \n",
    "            phase_name = \"Accurate\" if use_accurate else \"Approximate\"\n",
    "            print(f\"  [{phase_name}] {merge_iter+1}/{num_merges} ({percentage:.1f}%) | \"\n",
    "                  f\"Last merge: {merge_time:.1f}s (freq={best_count:,}) | \"\n",
    "                  f\"Avg rate: {rate:.2f}/s | ETA: {remaining/60:.1f}min\")\n",
    "        \n",
    "        # é˜¶æ®µåˆ‡æ¢æç¤º\n",
    "        if merge_iter == accurate_merges - 1:\n",
    "            phase_time = time.time() - phase_start\n",
    "            print(f\"\\n  âœ“ Accurate phase complete ({phase_time/60:.1f}min)\")\n",
    "            print(f\"  â†’ Switching to approximate mode (faster)...\\n\")\n",
    "            phase_start = time.time()\n",
    "    \n",
    "    total_merge_time = time.time() - (step_start - step_time)\n",
    "    print(f\"\\n  âœ“ All merges complete\")\n",
    "    print(f\"  âœ“ Time: {total_merge_time/60:.1f}min\\n\")\n",
    "    \n",
    "    # --------------------------\n",
    "    # 5ï¸âƒ£ ä¿å­˜æ¨¡å‹\n",
    "    # --------------------------\n",
    "    print(\"[Step 4/4] Saving model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    vocab_path = os.path.join(output_dir, \"vocab.json\")\n",
    "    merges_path = os.path.join(output_dir, \"merges.txt\")\n",
    "    \n",
    "    vocab_serialized = {str(k): v.decode(\"utf-8\", errors=\"replace\") for k, v in vocab.items()}\n",
    "    with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_serialized, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(merges_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b in merges:\n",
    "            f.write(f\"{a.decode('utf-8', errors='replace')} {b.decode('utf-8', errors='replace')}\\n\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  âœ“ Saved to {output_dir}\\n\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"âœ… BPE training completed!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Total time: {elapsed/60:.1f} minutes ({elapsed:.0f}s)\")\n",
    "    print(f\"  Final vocab size: {len(vocab)}\")\n",
    "    print(f\"  Total merges: {len(merges)}\")\n",
    "    print(f\"  Average rate: {len(merges)/elapsed:.2f} merges/s\")\n",
    "    print(f\"  Output directory: {output_dir}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d250fa9-7f90-49f5-8512-2503a20b55bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ Optimized Hybrid BPE Training\n",
      "================================================================================\n",
      "  Accurate updates: 60% of merges\n",
      "  Workers: 12\n",
      "  Recount interval: 100\n",
      "================================================================================\n",
      "\n",
      "[Step 1/3] Loading and tokenizing...\n",
      "  Tokenizing 21,990 lines with 12 workers (â‰¤12)...\n",
      "  âœ“ Loaded 21,990 stories, 4,620,113 tokens\n",
      "  âœ“ Time: -6.4s\n",
      "\n",
      "[Step 2/3] Counting initial pairs...\n",
      "  âœ“ Found 298,823 unique pairs\n",
      "  âœ“ Time: 2.2s\n",
      "\n",
      "[Step 3/3] Performing 9,743 merges\n",
      "  Phase 1: 5,845 accurate merges (parallel)\n",
      "  Phase 2: 3,898 approximate merges\n",
      "  Progress will be shown every 100 merges\n",
      "\n",
      "  [Accurate] 1/9743 (0.0%) | Last merge: 9.1s (freq=52,107) | Avg rate: 0.11/s | ETA: 1476.8min\n",
      "  [Accurate] 2/9743 (0.0%) | Last merge: -5.8s (freq=42,231) | Avg rate: 0.61/s | ETA: 267.9min\n",
      "  [Accurate] 3/9743 (0.0%) | Last merge: 1.9s (freq=40,692) | Avg rate: 0.57/s | ETA: 282.5min\n",
      "  [Accurate] 4/9743 (0.0%) | Last merge: 1.8s (freq=35,270) | Avg rate: 0.57/s | ETA: 283.5min\n",
      "  [Accurate] 5/9743 (0.1%) | Last merge: 1.5s (freq=22,576) | Avg rate: 0.59/s | ETA: 277.0min\n",
      "  [Accurate] 6/9743 (0.1%) | Last merge: 1.7s (freq=21,990) | Avg rate: 0.59/s | ETA: 275.7min\n",
      "  [Accurate] 7/9743 (0.1%) | Last merge: 8.9s (freq=21,990) | Avg rate: 0.37/s | ETA: 442.9min\n",
      "  [Accurate] 8/9743 (0.1%) | Last merge: -5.9s (freq=21,990) | Avg rate: 0.61/s | ETA: 267.8min\n",
      "  [Accurate] 9/9743 (0.1%) | Last merge: 1.7s (freq=20,610) | Avg rate: 0.60/s | ETA: 269.4min\n",
      "  [Accurate] 10/9743 (0.1%) | Last merge: 9.1s (freq=18,923) | Avg rate: 0.42/s | ETA: 389.9min\n",
      "  [Accurate] 100/9743 (1.0%) | Last merge: 1.5s (freq=2,646) | Avg rate: 0.67/s | ETA: 239.3min\n",
      "  [Accurate] 200/9743 (2.1%) | Last merge: 1.4s (freq=1,443) | Avg rate: 0.68/s | ETA: 233.9min\n",
      "  [Accurate] 300/9743 (3.1%) | Last merge: 8.9s (freq=988) | Avg rate: 0.69/s | ETA: 228.1min\n",
      "  [Accurate] 400/9743 (4.1%) | Last merge: 1.4s (freq=759) | Avg rate: 0.70/s | ETA: 222.1min\n",
      "  [Accurate] 500/9743 (5.1%) | Last merge: 1.3s (freq=642) | Avg rate: 0.71/s | ETA: 215.6min\n",
      "  [Accurate] 600/9743 (6.2%) | Last merge: 8.7s (freq=559) | Avg rate: 0.71/s | ETA: 214.7min\n",
      "  [Accurate] 700/9743 (7.2%) | Last merge: 1.3s (freq=476) | Avg rate: 0.72/s | ETA: 210.0min\n",
      "  [Accurate] 800/9743 (8.2%) | Last merge: 1.3s (freq=423) | Avg rate: 0.73/s | ETA: 204.4min\n",
      "  [Accurate] 900/9743 (9.2%) | Last merge: 1.3s (freq=374) | Avg rate: 0.74/s | ETA: 200.4min\n",
      "  [Accurate] 1000/9743 (10.3%) | Last merge: 1.7s (freq=338) | Avg rate: 0.74/s | ETA: 197.0min\n",
      "  [Accurate] 1100/9743 (11.3%) | Last merge: -6.2s (freq=312) | Avg rate: 0.74/s | ETA: 193.9min\n",
      "  [Accurate] 1200/9743 (12.3%) | Last merge: 1.3s (freq=283) | Avg rate: 0.74/s | ETA: 192.0min\n",
      "  [Accurate] 1300/9743 (13.3%) | Last merge: 1.2s (freq=261) | Avg rate: 0.75/s | ETA: 188.4min\n",
      "  [Accurate] 1400/9743 (14.4%) | Last merge: 1.4s (freq=242) | Avg rate: 0.75/s | ETA: 185.6min\n",
      "  [Accurate] 1500/9743 (15.4%) | Last merge: 1.2s (freq=224) | Avg rate: 0.75/s | ETA: 183.4min\n",
      "  [Accurate] 1600/9743 (16.4%) | Last merge: 1.3s (freq=209) | Avg rate: 0.75/s | ETA: 180.0min\n",
      "  [Accurate] 1700/9743 (17.4%) | Last merge: 1.2s (freq=196) | Avg rate: 0.76/s | ETA: 177.3min\n",
      "  [Accurate] 1800/9743 (18.5%) | Last merge: 1.2s (freq=185) | Avg rate: 0.76/s | ETA: 174.8min\n",
      "  [Accurate] 1900/9743 (19.5%) | Last merge: 1.4s (freq=175) | Avg rate: 0.76/s | ETA: 172.3min\n",
      "  [Accurate] 2000/9743 (20.5%) | Last merge: -6.2s (freq=166) | Avg rate: 0.76/s | ETA: 169.8min\n",
      "  [Accurate] 2100/9743 (21.6%) | Last merge: 1.2s (freq=158) | Avg rate: 0.76/s | ETA: 167.5min\n",
      "  [Accurate] 2200/9743 (22.6%) | Last merge: 1.3s (freq=150) | Avg rate: 0.76/s | ETA: 165.5min\n",
      "  [Accurate] 2300/9743 (23.6%) | Last merge: 1.3s (freq=144) | Avg rate: 0.76/s | ETA: 163.2min\n",
      "  [Accurate] 2400/9743 (24.6%) | Last merge: 8.8s (freq=138) | Avg rate: 0.76/s | ETA: 160.9min\n",
      "  [Accurate] 2500/9743 (25.7%) | Last merge: 1.5s (freq=132) | Avg rate: 0.76/s | ETA: 158.2min\n",
      "  [Accurate] 2600/9743 (26.7%) | Last merge: 1.3s (freq=127) | Avg rate: 0.76/s | ETA: 156.0min\n",
      "  [Accurate] 2700/9743 (27.7%) | Last merge: 1.3s (freq=123) | Avg rate: 0.76/s | ETA: 154.3min\n",
      "  [Accurate] 2800/9743 (28.7%) | Last merge: 1.5s (freq=119) | Avg rate: 0.76/s | ETA: 152.0min\n",
      "  [Accurate] 2900/9743 (29.8%) | Last merge: -6.3s (freq=114) | Avg rate: 0.76/s | ETA: 149.6min\n",
      "  [Accurate] 3000/9743 (30.8%) | Last merge: 1.3s (freq=111) | Avg rate: 0.76/s | ETA: 147.3min\n",
      "  [Accurate] 3100/9743 (31.8%) | Last merge: 1.5s (freq=106) | Avg rate: 0.76/s | ETA: 145.1min\n",
      "  [Accurate] 3200/9743 (32.8%) | Last merge: 1.3s (freq=102) | Avg rate: 0.76/s | ETA: 143.2min\n",
      "  [Accurate] 3300/9743 (33.9%) | Last merge: 1.3s (freq=99) | Avg rate: 0.76/s | ETA: 141.1min\n",
      "  [Accurate] 3400/9743 (34.9%) | Last merge: 1.4s (freq=96) | Avg rate: 0.76/s | ETA: 138.9min\n",
      "  [Accurate] 3500/9743 (35.9%) | Last merge: 1.3s (freq=93) | Avg rate: 0.76/s | ETA: 136.7min\n",
      "  [Accurate] 3600/9743 (36.9%) | Last merge: 1.2s (freq=90) | Avg rate: 0.76/s | ETA: 134.6min\n",
      "  [Accurate] 3700/9743 (38.0%) | Last merge: 1.4s (freq=87) | Avg rate: 0.76/s | ETA: 132.2min\n",
      "  [Accurate] 3800/9743 (39.0%) | Last merge: 1.4s (freq=85) | Avg rate: 0.76/s | ETA: 130.0min\n",
      "  [Accurate] 3900/9743 (40.0%) | Last merge: 1.3s (freq=83) | Avg rate: 0.76/s | ETA: 127.9min\n",
      "  [Accurate] 4000/9743 (41.1%) | Last merge: 1.2s (freq=81) | Avg rate: 0.76/s | ETA: 125.8min\n",
      "  [Accurate] 4100/9743 (42.1%) | Last merge: -6.3s (freq=78) | Avg rate: 0.76/s | ETA: 123.7min\n",
      "  [Accurate] 4200/9743 (43.1%) | Last merge: 1.3s (freq=76) | Avg rate: 0.76/s | ETA: 121.5min\n",
      "  [Accurate] 4300/9743 (44.1%) | Last merge: 1.3s (freq=74) | Avg rate: 0.76/s | ETA: 119.5min\n",
      "  [Accurate] 4400/9743 (45.2%) | Last merge: 1.4s (freq=72) | Avg rate: 0.76/s | ETA: 117.2min\n",
      "  [Accurate] 4500/9743 (46.2%) | Last merge: 1.3s (freq=71) | Avg rate: 0.76/s | ETA: 115.1min\n",
      "  [Accurate] 4600/9743 (47.2%) | Last merge: 1.3s (freq=69) | Avg rate: 0.76/s | ETA: 113.1min\n",
      "  [Accurate] 4700/9743 (48.2%) | Last merge: 9.1s (freq=67) | Avg rate: 0.76/s | ETA: 111.0min\n",
      "  [Accurate] 4800/9743 (49.3%) | Last merge: 1.3s (freq=66) | Avg rate: 0.76/s | ETA: 108.8min\n",
      "  [Accurate] 4900/9743 (50.3%) | Last merge: -6.2s (freq=65) | Avg rate: 0.76/s | ETA: 106.6min\n",
      "  [Accurate] 5000/9743 (51.3%) | Last merge: 1.6s (freq=63) | Avg rate: 0.76/s | ETA: 104.4min\n",
      "  [Accurate] 5100/9743 (52.3%) | Last merge: 1.3s (freq=62) | Avg rate: 0.76/s | ETA: 102.3min\n",
      "  [Accurate] 5200/9743 (53.4%) | Last merge: 1.4s (freq=61) | Avg rate: 0.76/s | ETA: 100.1min\n",
      "  [Accurate] 5300/9743 (54.4%) | Last merge: 1.4s (freq=59) | Avg rate: 0.76/s | ETA: 98.1min\n",
      "  [Accurate] 5400/9743 (55.4%) | Last merge: 1.3s (freq=58) | Avg rate: 0.75/s | ETA: 95.9min\n",
      "  [Accurate] 5500/9743 (56.5%) | Last merge: 1.4s (freq=57) | Avg rate: 0.75/s | ETA: 93.7min\n",
      "  [Accurate] 5600/9743 (57.5%) | Last merge: 8.8s (freq=56) | Avg rate: 0.75/s | ETA: 91.7min\n",
      "  [Accurate] 5700/9743 (58.5%) | Last merge: 1.7s (freq=55) | Avg rate: 0.75/s | ETA: 89.5min\n",
      "  [Accurate] 5800/9743 (59.5%) | Last merge: 1.4s (freq=54) | Avg rate: 0.75/s | ETA: 87.4min\n",
      "\n",
      "  âœ“ Accurate phase complete (129.7min)\n",
      "  â†’ Switching to approximate mode (faster)...\n",
      "\n",
      "  [Recount at merge 5845] Time: 3.7s\n",
      "  [Approximate] 5900/9743 (60.6%) | Last merge: 0.1s (freq=53) | Avg rate: 423.84/s | ETA: 0.2min\n",
      "  [Recount at merge 5945] Time: 11.7s\n",
      "  [Approximate] 6000/9743 (61.6%) | Last merge: -7.3s (freq=53) | Avg rate: 156.53/s | ETA: 0.4min\n",
      "  [Recount at merge 6045] Time: 11.2s\n",
      "  [Approximate] 6100/9743 (62.6%) | Last merge: 0.2s (freq=53) | Avg rate: 94.29/s | ETA: 0.6min\n",
      "  [Recount at merge 6145] Time: 3.9s\n",
      "  [Approximate] 6200/9743 (63.6%) | Last merge: 0.3s (freq=53) | Avg rate: 67.91/s | ETA: 0.9min\n",
      "  [Recount at merge 6245] Time: 3.1s\n",
      "  [Approximate] 6300/9743 (64.7%) | Last merge: 0.1s (freq=53) | Avg rate: 55.26/s | ETA: 1.0min\n",
      "  [Recount at merge 6345] Time: 3.8s\n",
      "  [Approximate] 6400/9743 (65.7%) | Last merge: 0.3s (freq=53) | Avg rate: 45.30/s | ETA: 1.2min\n",
      "  [Recount at merge 6445] Time: 3.8s\n",
      "  [Approximate] 6500/9743 (66.7%) | Last merge: 0.2s (freq=53) | Avg rate: 38.35/s | ETA: 1.4min\n",
      "  [Recount at merge 6545] Time: -3.6s\n",
      "  [Approximate] 6600/9743 (67.7%) | Last merge: 0.4s (freq=53) | Avg rate: 32.50/s | ETA: 1.6min\n",
      "  [Recount at merge 6645] Time: -4.1s\n",
      "  [Approximate] 6700/9743 (68.8%) | Last merge: 0.2s (freq=53) | Avg rate: 30.02/s | ETA: 1.7min\n",
      "  [Recount at merge 6745] Time: 3.6s\n",
      "  [Approximate] 6800/9743 (69.8%) | Last merge: 8.0s (freq=53) | Avg rate: 26.53/s | ETA: 1.8min\n",
      "  [Recount at merge 6845] Time: 3.3s\n",
      "  [Approximate] 6900/9743 (70.8%) | Last merge: 0.2s (freq=53) | Avg rate: 25.24/s | ETA: 1.9min\n",
      "  [Recount at merge 6945] Time: 3.4s\n",
      "  [Approximate] 7000/9743 (71.8%) | Last merge: 0.2s (freq=53) | Avg rate: 22.81/s | ETA: 2.0min\n",
      "  [Recount at merge 7045] Time: 3.3s\n",
      "  [Approximate] 7100/9743 (72.9%) | Last merge: 0.2s (freq=53) | Avg rate: 21.90/s | ETA: 2.0min\n",
      "  [Recount at merge 7145] Time: 3.3s\n",
      "  [Approximate] 7200/9743 (73.9%) | Last merge: 0.2s (freq=53) | Avg rate: 20.62/s | ETA: 2.1min\n",
      "  [Recount at merge 7245] Time: 3.0s\n",
      "  [Approximate] 7300/9743 (74.9%) | Last merge: 0.2s (freq=53) | Avg rate: 19.22/s | ETA: 2.1min\n",
      "  [Recount at merge 7345] Time: 3.6s\n",
      "  [Approximate] 7400/9743 (76.0%) | Last merge: 0.3s (freq=53) | Avg rate: 18.63/s | ETA: 2.1min\n",
      "  [Recount at merge 7445] Time: 3.1s\n",
      "  [Approximate] 7500/9743 (77.0%) | Last merge: 0.2s (freq=53) | Avg rate: 17.80/s | ETA: 2.1min\n",
      "  [Recount at merge 7545] Time: 4.0s\n",
      "  [Approximate] 7600/9743 (78.0%) | Last merge: 0.2s (freq=53) | Avg rate: 17.01/s | ETA: 2.1min\n",
      "  [Recount at merge 7645] Time: 3.2s\n",
      "  [Approximate] 7700/9743 (79.0%) | Last merge: 0.2s (freq=53) | Avg rate: 16.36/s | ETA: 2.1min\n",
      "  [Recount at merge 7745] Time: 3.6s\n",
      "  [Approximate] 7800/9743 (80.1%) | Last merge: 0.2s (freq=53) | Avg rate: 15.74/s | ETA: 2.1min\n",
      "  [Recount at merge 7845] Time: 3.0s\n",
      "  [Approximate] 7900/9743 (81.1%) | Last merge: 0.2s (freq=53) | Avg rate: 15.00/s | ETA: 2.0min\n",
      "  [Recount at merge 7945] Time: -3.7s\n",
      "  [Approximate] 8000/9743 (82.1%) | Last merge: 0.4s (freq=53) | Avg rate: 14.45/s | ETA: 2.0min\n",
      "  [Recount at merge 8045] Time: 3.4s\n",
      "  [Approximate] 8100/9743 (83.1%) | Last merge: 0.2s (freq=53) | Avg rate: 13.96/s | ETA: 2.0min\n",
      "  [Recount at merge 8145] Time: -4.3s\n",
      "  [Approximate] 8200/9743 (84.2%) | Last merge: 0.3s (freq=53) | Avg rate: 13.72/s | ETA: 1.9min\n",
      "  [Recount at merge 8245] Time: 3.2s\n",
      "  [Approximate] 8300/9743 (85.2%) | Last merge: 0.2s (freq=53) | Avg rate: 13.32/s | ETA: 1.8min\n",
      "  [Recount at merge 8345] Time: 3.6s\n",
      "  [Approximate] 8400/9743 (86.2%) | Last merge: 0.2s (freq=53) | Avg rate: 12.79/s | ETA: 1.8min\n",
      "  [Recount at merge 8445] Time: 2.9s\n",
      "  [Approximate] 8500/9743 (87.2%) | Last merge: 0.1s (freq=53) | Avg rate: 12.48/s | ETA: 1.7min\n",
      "  [Recount at merge 8545] Time: -4.1s\n",
      "  [Approximate] 8600/9743 (88.3%) | Last merge: 0.2s (freq=53) | Avg rate: 12.33/s | ETA: 1.5min\n",
      "  [Recount at merge 8645] Time: 2.9s\n",
      "  [Approximate] 8700/9743 (89.3%) | Last merge: 0.2s (freq=53) | Avg rate: 12.07/s | ETA: 1.4min\n",
      "  [Recount at merge 8745] Time: 3.3s\n",
      "  [Approximate] 8800/9743 (90.3%) | Last merge: 0.2s (freq=53) | Avg rate: 11.83/s | ETA: 1.3min\n",
      "  [Recount at merge 8845] Time: 2.9s\n",
      "  [Approximate] 8900/9743 (91.3%) | Last merge: 0.1s (freq=53) | Avg rate: 11.59/s | ETA: 1.2min\n",
      "  [Recount at merge 8945] Time: 3.1s\n",
      "  [Approximate] 9000/9743 (92.4%) | Last merge: 0.2s (freq=53) | Avg rate: 11.37/s | ETA: 1.1min\n",
      "  [Recount at merge 9045] Time: 3.5s\n",
      "  [Approximate] 9100/9743 (93.4%) | Last merge: 0.1s (freq=53) | Avg rate: 11.14/s | ETA: 1.0min\n",
      "  [Recount at merge 9145] Time: 3.2s\n",
      "  [Approximate] 9200/9743 (94.4%) | Last merge: 0.3s (freq=53) | Avg rate: 10.84/s | ETA: 0.8min\n",
      "  [Recount at merge 9245] Time: 3.1s\n",
      "  [Approximate] 9300/9743 (95.5%) | Last merge: 0.2s (freq=53) | Avg rate: 10.66/s | ETA: 0.7min\n",
      "  [Recount at merge 9345] Time: -4.3s\n",
      "  [Approximate] 9400/9743 (96.5%) | Last merge: 0.2s (freq=53) | Avg rate: 10.49/s | ETA: 0.5min\n",
      "  [Recount at merge 9445] Time: 3.1s\n",
      "  [Approximate] 9500/9743 (97.5%) | Last merge: 0.2s (freq=53) | Avg rate: 10.31/s | ETA: 0.4min\n",
      "  [Recount at merge 9545] Time: 3.3s\n",
      "  [Approximate] 9600/9743 (98.5%) | Last merge: 0.2s (freq=53) | Avg rate: 10.22/s | ETA: 0.2min\n",
      "  [Recount at merge 9645] Time: 2.9s\n",
      "  [Approximate] 9700/9743 (99.6%) | Last merge: 0.3s (freq=53) | Avg rate: 9.99/s | ETA: 0.1min\n",
      "\n",
      "  âœ“ All merges complete\n",
      "  âœ“ Time: 146.0min\n",
      "\n",
      "[Step 4/4] Saving model...\n",
      "  âœ“ Saved to ./bpe_model_hybrid\n",
      "\n",
      "================================================================================\n",
      "âœ… BPE training completed!\n",
      "================================================================================\n",
      "  Total time: 145.8 minutes (8750s)\n",
      "  Final vocab size: 10000\n",
      "  Total merges: 9743\n",
      "  Average rate: 1.11 merges/s\n",
      "  Output directory: ./bpe_model_hybrid\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = \"/home/winbeau/Study/1-transformer/datasets/TinyStories/txt/valid_with_eot.txt\"\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "vocab, merges = train_bpe(\n",
    "    input_path=train_path,\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    PAT=PAT,\n",
    "    output_dir=\"./bpe_model_hybrid\",\n",
    "    accurate_ratio=0.6,  # å‰60%ç²¾ç¡®ï¼Œå40%è¿‘ä¼¼\n",
    "    num_proc=12, \n",
    "    recount_interval=100, # æ§åˆ¶æœ€åè¿‘ä¼¼æ›´æ–°çš„é‡æ–°ç»Ÿè®¡æ›´æ–°é¢‘ç‡ | é»˜è®¤100æ¬¡\n",
    "    progress_interval=100  # æ¯100æ¬¡mergeæ˜¾ç¤ºä¸€æ¬¡ | å‰10æ¬¡å¿…æ˜¾ç¤º\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f7cfd-ea56-4c3c-871e-1aab8e837801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
