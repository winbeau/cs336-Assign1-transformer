{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e461c3c6-5c1b-4dab-84ee-a49048bd68d8",
   "metadata": {},
   "source": [
    "## 1-数据预处理 | 为训练数据添加 `<|endoftext|>`\n",
    "- 将训练集中每个故事添加结束标志，防止故事间跨越，学习全局超长语料\n",
    "- 依然是全局BPE，但是故事间有 `<|endoftext|>` 作为挡板\n",
    "- **不是** 每个故事单独训练一个BPE词表再汇总\\[这可能会导致出现不同的token编号，模型无法同一使用\\]，而是全局统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df417fc4-585a-4c13-b0eb-4ec5da7822ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_path = \"/home/winbeau/Study/1-transformer/datasets/TinyStories/txt/train.txt\"\n",
    "valid_raw_path = \"/home/winbeau/Study/1-transformer/datasets/TinyStories/txt/valid.txt\"\n",
    "\n",
    "# 查看前几行\n",
    "def preview_txt(cnt_line, txt_path=train_raw_path): \n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f): # 行号 和 内容\n",
    "            if i >= cnt_line:\n",
    "                break\n",
    "            print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24a99f7-557d-4353-b299-4b322e537e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.  Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"  Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.  One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.  Beep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.\n"
     ]
    }
   ],
   "source": [
    "preview_txt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6cd50c3-d11b-42fe-9ada-1afd366d618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "base_dir = \"/home/winbeau/Study/1-transformer/datasets/TinyStories/txt/\"\n",
    "\n",
    "files = [\n",
    "    (\"train.txt\", \"train_with_eot.txt\"), \n",
    "    (\"valid.txt\", \"valid_with_eot.txt\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc43d3a7-0635-4425-a6ef-351752053d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_endoftext(infile: str, outfile: str): \n",
    "    \"\"\"行末添加 <|endoftext|> , 忽略空行 \"\"\"\n",
    "    cnt_in, cnt_out = 0, 0\n",
    "    with open(infile, \"r\", encoding=\"utf-8\") as fin, open(outfile, \"w\", encoding=\"utf=8\") as fout: \n",
    "        for line in fin: \n",
    "            text = line.strip()\n",
    "            cnt_in += 1\n",
    "            if text: \n",
    "                fout.write(text + \"<|endoftext|>\\n\")\n",
    "                cnt_out += 1\n",
    "    print(f\"Complete! {cnt_out} / {cnt_in}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41fd379f-e0c8-410f-af04-61db53a73100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete! 2119489 / 2119719\n",
      "Complete! 21990 / 21990\n",
      "All files have added <|endoftext|> and saved!\n"
     ]
    }
   ],
   "source": [
    "for fin, fout in files: \n",
    "    add_endoftext(\n",
    "        os.path.join(base_dir, fin),\n",
    "        os.path.join(base_dir, fout) \n",
    "    )\n",
    "print(\"All files have added <|endoftext|> and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ad39182-d318-4042-8482-5404ceada12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.  Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"  Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.<|endoftext|>\n",
      "Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.  One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.  Beep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "preview_txt(2, os.path.join(base_dir, \"train_with_eot.txt\")) # 验证是否加入成功"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b991a72-3456-4424-9d3d-aff1155f46ab",
   "metadata": {},
   "source": [
    "## 2-BPE | 预分词(Pre-tokenization) \\[ 多进程并行 \\]\n",
    "<input type=\"checkbox\" unchecked> BPE-No.1: 预分词(Pre-tokenization) 把原始文本分成初步\"词形片段\"并计数(正则化去掉标点、符号)\n",
    "\n",
    "<input type=\"checkbox\" unchecked> BPE-No.2: 统计字节对频率(Pair counting) 在\"词形片段\"内部把相邻字节两两匹配、计算出现频率\n",
    "\n",
    "<input type=\"checkbox\" unchecked> BPE-No.3: 合并(Merge) 选取最高频(并列选最高字典序) pair 合并成新 token 重复到 vocab 满"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66bc4d8a-1d5e-4e1d-af33-89bad1e6096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re \n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from collections import Counter\n",
    "from cs336_pretokenization_example import find_chunk_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97bebf5-b92c-453c-8fd7-8b05ce70ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/winbeau/Study/1-transformer/datasets/TinyStories/txt/train_with_eot.txt\"\n",
    "assert os.path.exists(train_path), \"Not found train_with_eot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9703bdce-b383-4535-b788-5a9f46ea3b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 processes\n"
     ]
    }
   ],
   "source": [
    "num_processes = min(12, cpu_count())\n",
    "print(f\"Using {num_processes} processes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3abd55-31bb-401d-bd13-b2c54a67f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex 正则化 减弱标点、其他符号对文本的影响\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "SPECIAL = \"<|endoftext|>\" # 结束标志特殊正则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f897b09f-ef99-4d35-92cb-1dbe0b028981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"不推荐使用这个函数 split 会把原始字符串复制一份\"\"\"\n",
    "def pretokenize_chunk(chunk: str): # 如果不对结尾特殊正则 会出现 .<| 与 |>\n",
    "    tokens = []\n",
    "    parts = chunk.split(SPECIAL) # 这将导致内存直接翻倍\n",
    "    for i, part in enumerate(parts): \n",
    "        for match in re.finditer(PAT, part): # 普通文本一般正则\n",
    "            tokens.append(match.group())\n",
    "        if i < len(parts) - 1: # 最后一段特殊正则 SPECIAL = \"<|endoftext|>\" \n",
    "            tokens.append(SPECIAL)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21cbc7e-3b89-4456-b69c-e67871b75c1c",
   "metadata": {},
   "source": [
    "`file.seek()` 把文件指针跳到当前块的**起点**\n",
    "\n",
    "`file.read()` 从**起点**读取当前块的长度\n",
    "\n",
    "`Counter()` 自动计数器\n",
    "\n",
    "`group()` 显示分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "089d8944-90a2-4eac-ba30-10bde13299cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_(start_end): # 进程处理函数 对 <|endoftext|> 匹配失效\n",
    "    start, end = start_end \n",
    "    counter = Counter()\n",
    "\n",
    "    with open(train_path, \"rb\") as f: # 二进制读取 精确跳跃位置\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    for match in re.finditer(PAT, chunk): # 正则分词\n",
    "        tokens = match.group()\n",
    "        counter[tokens.encode(\"utf-8\")] += 1 # 统计为 Unicode bytes\n",
    "\n",
    "    return counter \n",
    "\n",
    "def process_chunk(start_end):\n",
    "    start, end = start_end\n",
    "    counter = Counter()\n",
    "\n",
    "    with open(train_path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    idx = 0\n",
    "    while True: # 手动查找 <|endoftext|> 位置\n",
    "        pos = chunk.find(SPECIAL, idx)\n",
    "        if pos == -1: # 没找到 对剩下部分用正则分词\n",
    "            part = chunk[idx:]\n",
    "            for m in re.finditer(PAT, part):\n",
    "                tok = m.group()\n",
    "                counter[tok.encode(\"utf-8\")] += 1\n",
    "            break\n",
    "        # 对特殊 token 前面的部分做分词\n",
    "        part = chunk[idx:pos]\n",
    "        for m in re.finditer(PAT, part):\n",
    "            tok = m.group()\n",
    "            counter[tok.encode(\"utf-8\")] += 1\n",
    "        # 单独计一次特殊 token\n",
    "        counter[SPECIAL.encode(\"utf-8\")] += 1\n",
    "        idx = pos + len(SPECIAL)\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "114dade0-baa0-46b6-a5d6-ab701cf3c1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 chunks\n"
     ]
    }
   ],
   "source": [
    "with open(train_path, \"rb\") as f: \n",
    "    boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "chunk_pairs = list(zip(boundaries[:-1], boundaries[1:])) # 0 1 -> 1 2 \n",
    "print(f\"Found {len(chunk_pairs)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af58037c-5cb4-4df1-9986-625519d20d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-tokenization chunks: 100%|██████████████████| 12/12 [00:57<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens: 72600\n"
     ]
    }
   ],
   "source": [
    "with Pool(num_processes) as p: \n",
    "    counters = list(tqdm(\n",
    "        p.imap(process_chunk, chunk_pairs), # 若不想加过程可视化模块直接 p.imap 即可\n",
    "        total=len(chunk_pairs), \n",
    "        desc=\"Pre-tokenization chunks\", \n",
    "        ncols=80\n",
    "    ))\n",
    "\n",
    "total_counts = sum(counters, Counter())\n",
    "print(f\"Total unique tokens: {len(total_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6b9c95a-e41e-43e9-9421-4b9a2cc58e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common tokens:\n",
      "'.' : 34641640 \n",
      "' and' : 17810182 \n",
      "',' : 16629494 \n",
      "' the' : 16600232 \n",
      "' to' : 12630263 \n",
      "' a' : 11358991 \n",
      "' was' : 9435962 \n",
      "' ' : 7521334 \n",
      "' it' : 5172631 \n",
      "' \"' : 5002578 \n",
      "<|endoftext|> freq: 2119489\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 most common tokens:\") # 可以看到有很多前导空格————' '频率极高，使用前导空格优化效率\n",
    "for token, freq in total_counts.most_common(10): \n",
    "    try: \n",
    "        print(f\"{token.decode('utf-8', errors='ignore')!r} : {freq} \")\n",
    "    except Exception: \n",
    "        print(f\"{token} : {freq}\")\n",
    "print(f\"<|endoftext|> freq: {total_counts.get(b'<|endoftext|>', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c414f2-9077-48f8-b5e6-948c6d5ee84c",
   "metadata": {},
   "source": [
    "## 3-BPE | 统计字节对频率(Pair counting)\n",
    "<input type=\"checkbox\" checked> BPE-No.1: 预分词(Pre-tokenization) 把原始文本分成初步\"词形片段\"并计数(正则化去掉标点、符号)\n",
    "\n",
    "<input type=\"checkbox\" unchecked> BPE-No.2: 统计字节对频率(Pair counting) 在\"词形片段\"内部把相邻字节两两匹配、计算出现频率\n",
    "\n",
    "<input type=\"checkbox\" unchecked> BPE-No.3: 合并(Merge) 选取最高频(并列选最高字典序) pair 合并成新 token 重复到 vocab 满"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423dd9d2-1371-4677-84e4-314958765dfc",
   "metadata": {},
   "source": [
    "现有 $n$ 个变长字符串 $str_i$ ($len(str_i) \\leq 20,~i \\in [0, \\, n)$)，总长度 $\\sum_{i = 0}^n len(str_i) \\leq 150 \\times 2 \\times 10^6 = 3 \\times 10^8$，\n",
    "\n",
    "$vocab$ 为 $token ~ id$ 对 $Unicode ~ byte(s)$ 的映射集合（也就是 $token$ 集合），初始化为 $[0, 256)$ 对应其十六进制数所代表的 $Unicode ~ byte$ (可以理解为 $ASCII ~ plus$) \n",
    "\n",
    "**持续执行如下操作：**\n",
    "\n",
    "对每个字符串 ***所有最长 $token$*** 进行两两结合统计频率：\n",
    "\n",
    "<aside>\n",
    "\n",
    "初始状态每个字符是一个 $token$\n",
    "\n",
    "`word` → `wo` : 1  `or` : 1  `rd` : 1\n",
    "\n",
    "> 什么是***最长 $token$*** ？如 `newest` ，`est` 已分配 $token ~ id$，我们仅对 `ne` `ew` `west` 处理\n",
    "> \n",
    "</aside>\n",
    "\n",
    "将频率最高的一个 $pair$ （若并列则取最高字典序）分配新的 $token ~ id$ \n",
    "\n",
    "比如上述 `word` 则新分配 `279` → `wo`\n",
    "\n",
    "**截止状态：**\n",
    "\n",
    "最终整个序列已经分配了 $token ~ id$（几乎不可能） 或 $token ~ id$ 的数目 = $vocab\\_size$\n",
    "\n",
    "$token ~ id \\leq vocab\\_size = 10^4$\n",
    "\n",
    "---\n",
    "\n",
    "#### 输入\n",
    "\n",
    "二维字符串数组\n",
    "\n",
    "#### 输出\n",
    "\n",
    "$vocab$ : 从 $token ~ id$ 到 $bytes$ 的映射\n",
    "\n",
    "$merges$ : 产生的 合并 $pair$（按创建顺序排列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1102d198-c02d-43a6-92a0-eb4e0540d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple \n",
    "\n",
    "vocab_size = 10000\n",
    "special_tokens = [\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3c880-276d-40a6-a184-4f2c48c1c89e",
   "metadata": {},
   "source": [
    "#### train_bpe 需要统计全局 pair，但是读取整个文件内存会爆炸，采取策略：\n",
    "- 全局视野，分块统计\n",
    "- 只存频率，不存数据\n",
    "- 一次统计，多次更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b406f8a-ecc0-488f-b500-6204d5f0f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, regex\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_chunk_freq(args):\n",
    "    \"\"\"单个进程：读取一块 -> 正则分词 -> 统计 pair 频率\"\"\"\n",
    "    input_path, start, end, special_tokens = args\n",
    "    local_counter = Counter()\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        text = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # 按正则模式分词\n",
    "    for token in regex.findall(PAT, text):\n",
    "        token = token.strip()\n",
    "        if not token or token in special_tokens:\n",
    "            continue\n",
    "        seq = list(token.encode(\"utf-8\"))\n",
    "        local_counter.update(zip(seq, seq[1:]))\n",
    "\n",
    "    return local_counter\n",
    "\n",
    "\n",
    "def train_bpe_freq(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str],\n",
    ") -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n",
    "\n",
    "    from cs336_pretokenization_example import find_chunk_boundaries  # 你自己写的函数\n",
    "\n",
    "    num_proc = min(2, cpu_count())\n",
    "    print(f\"Using {num_proc} processes...\")\n",
    "\n",
    "    # === Step 1️⃣: 按 <|endoftext|> 切块 ===\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_proc, b\"<|endoftext|>\")\n",
    "    chunk_pairs = list(zip(boundaries[:-1], boundaries[1:]))\n",
    "\n",
    "    # === Step 2️⃣: 并行统计 Counter ===\n",
    "    with Pool(num_proc) as pool:\n",
    "        counters = list(tqdm(\n",
    "            pool.imap(process_chunk_freq,\n",
    "                      [(input_path, s, e, special_tokens) for s, e in chunk_pairs]),\n",
    "            total=len(chunk_pairs),\n",
    "            desc=\"Pre-tokenization + Counting\",\n",
    "            ncols=80\n",
    "        ))\n",
    "\n",
    "    # === Step 3️⃣: 汇总所有 pair 频率 ===\n",
    "    total_pairs = Counter()\n",
    "    for c in counters:\n",
    "        total_pairs.update(c)\n",
    "\n",
    "    # === Step 4️⃣: 初始化词表 ===\n",
    "    vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}\n",
    "    merges: List[Tuple[bytes, bytes]] = []\n",
    "    next_id = 256\n",
    "    for tok in special_tokens:\n",
    "        vocab[next_id] = tok.encode(\"utf-8\")\n",
    "        next_id += 1\n",
    "\n",
    "    # === Step 5️⃣: BPE 训练循环 ===\n",
    "    pbar = tqdm(total=vocab_size - next_id, desc=\"Training BPE\", ncols=80)\n",
    "    while next_id < vocab_size and total_pairs:\n",
    "        best_pair, freq = total_pairs.most_common(1)[0]\n",
    "        if freq < 2:\n",
    "            break\n",
    "\n",
    "        # 注册新 token\n",
    "        new_bytes = vocab[best_pair[0]] + vocab[best_pair[1]]\n",
    "        vocab[next_id] = new_bytes\n",
    "        merges.append((vocab[best_pair[0]], vocab[best_pair[1]]))\n",
    "        next_id += 1\n",
    "\n",
    "        # 简单版：下一轮重新统计所有 pair（保证正确）\n",
    "        # 对 10k vocab + TinyStories 可接受\n",
    "        total_pairs = Counter()\n",
    "        with Pool(num_proc) as pool:\n",
    "            counters = list(pool.imap(\n",
    "                process_chunk_freq,\n",
    "                [(input_path, s, e, special_tokens) for s, e in chunk_pairs],\n",
    "            ))\n",
    "        for c in counters:\n",
    "            total_pairs.update(c)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083be46f-214a-4072-917d-a4a826bd440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-tokenization + Counting:   0%|                        | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_path = \"/home/winbeau/Study/1-transformer/datasets/TinyStories/txt/train_with_eot.txt\"\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "\n",
    "vocab, merges = train_bpe_freq(train_path, vocab_size=10000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a790d522-b789-422a-a35b-0d11b6530921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97, 99]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"ac\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab546ef-d9c5-4f8a-9dd0-b9915ff75326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CS336)",
   "language": "python",
   "name": "cs336"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
