{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70cab29-c686-468e-b184-e63e63581df6",
   "metadata": {},
   "source": [
    "# CS336 Assignment 1 — Training Functions\n",
    "\n",
    "本 notebook 定义训练循环需要的所有基础组件：\n",
    "\n",
    "* `cross_entropy_loss`\n",
    "* `AdamW` (自己写的 Optimizer，不能用 torch.optim.AdamW)\n",
    "* 学习率调度器：余弦退火 + warmup\n",
    "* 梯度裁剪\n",
    "* batch 抽样（`get_batch`）\n",
    "* checkpoint 的保存 / 加载\n",
    "* `TrainingConfig` (方便后面主训练 loop 用)\n",
    "\n",
    "这些实现遵守作业要求：\n",
    "\n",
    "* 不调用 `torch.nn.functional`、`torch.optim` 里的现成实现（除了基类 Optimizer）\n",
    "* 注意数值稳定性（logits 减最大值）\n",
    "* 注意跨设备（我们会传 `device=\"cuda\"`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c0661c-a74f-4184-8043-881d56c774ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Iterable, Tuple, List\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f9966-5c23-4486-8d98-300ac943b9ee",
   "metadata": {},
   "source": [
    "## 1. cross_entropy_loss\n",
    "\n",
    "交叉熵（单步）定义：\n",
    "ℓ = -log softmax(logits)[target]\n",
    "\n",
    "数值稳定写法：\n",
    "\n",
    "* 减去最大值 `m = logits.max(dim=-1)`\n",
    "* `logsumexp = m + log(sum(exp(logits - m)))`\n",
    "* 目标 token 的 log-prob = logits[gather] - logsumexp\n",
    "* 取负号并对 batch 取平均\n",
    "\n",
    "注意：\n",
    "\n",
    "* logits shape: `(..., vocab_size)`\n",
    "* targets shape: `(...)` (同 batch 维，但少了 vocab_size 那一维)\n",
    "* 我们要返回标量 loss（平均）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b33767-76a8-4a6c-b87b-e57855cea811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute average cross-entropy over all batch positions.\n",
    "\n",
    "    logits: (..., vocab_size) float32/float16/bfloat16\n",
    "    targets: (...) int64, same leading shape as logits without vocab dim\n",
    "\n",
    "    returns: scalar tensor (mean loss)\n",
    "    \"\"\"\n",
    "    # keep original dtype for later? no need, loss is float32 typically\n",
    "    # We'll upcast to float32 for numerical stability.\n",
    "    logits_f32 = logits.to(torch.float32)\n",
    "\n",
    "    # max over vocab dimension (last dim)\n",
    "    max_logits, _ = torch.max(logits_f32, dim=-1, keepdim=True)  # (..., 1)\n",
    "\n",
    "    # shift for stability\n",
    "    shifted = logits_f32 - max_logits  # (..., vocab)\n",
    "\n",
    "    # logsumexp = log(sum(exp(shifted)))\n",
    "    sum_exp = torch.sum(torch.exp(shifted), dim=-1, keepdim=False)  # (...)\n",
    "    logsumexp = torch.log(sum_exp) + max_logits.squeeze(-1)        # (...)\n",
    "\n",
    "    # gather the logit of the true target\n",
    "    # targets shape (...) -> need to align\n",
    "    true_logits = torch.take_along_dim(\n",
    "        logits_f32, targets.unsqueeze(-1), dim=-1\n",
    "    ).squeeze(-1)  # (...)\n",
    "\n",
    "    # nll = -(true_logit - logsumexp)\n",
    "    nll = -(true_logits - logsumexp)  # (...,)\n",
    "\n",
    "    # average over all positions in batch\n",
    "    loss = nll.mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1755474b-0bda-48b1-8ac6-e93e850cdecf",
   "metadata": {},
   "source": [
    "## 2. 自定义 AdamW\n",
    "\n",
    "按照作业算法：\n",
    "\n",
    "* 维护一阶动量 m、二阶动量 v（存在 `self.state[p]`）\n",
    "* 偏置修正：\n",
    "  α_t = α * sqrt(1-β2^t)/(1-β1^t)\n",
    "* 参数更新：\n",
    "  θ ← θ - α_t * m / (sqrt(v)+eps)\n",
    "  然后再做权重衰减：θ ← θ - α * weight_decay * θ\n",
    "\n",
    "注意：\n",
    "\n",
    "* 我们用 `torch.optim.Optimizer` 作为基类（这是允许的）\n",
    "* 不允许用现成的 Adam/AdamW\n",
    "* 我们也实现 `zero_grad()` 的惯用法：就用父类里的 `optimizer.zero_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ddc058-eb5f-4bd7-80b3-3fa67c6c9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Minimal AdamW optimizer from scratch (decoupled weight decay).\n",
    "    Matches the algorithm described in the handout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.95),\n",
    "        eps: float = 1e-8,\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if eps < 0:\n",
    "            raise ValueError(f\"Invalid eps: {eps}\")\n",
    "        if weight_decay < 0:\n",
    "            raise ValueError(f\"Invalid weight_decay: {weight_decay}\")\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        # Iterate over param groups\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            beta1, beta2 = group[\"betas\"]\n",
    "            eps = group[\"eps\"]\n",
    "            wd = group[\"weight_decay\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"AdamW does not support sparse gradients\")\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg = state[\"exp_avg\"]\n",
    "                exp_avg_sq = state[\"exp_avg_sq\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "                t = state[\"step\"]\n",
    "\n",
    "                # Update biased first moment estimate\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=(1 - beta1))\n",
    "                # Update biased second raw moment estimate\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=(1 - beta2))\n",
    "\n",
    "                # Bias correction\n",
    "                bias_correction1 = 1 - beta1 ** t\n",
    "                bias_correction2 = 1 - beta2 ** t\n",
    "                # Compute step size\n",
    "                step_size = lr * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                # denom = sqrt(v) + eps\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "\n",
    "                # Adam update\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "                # Decoupled weight decay\n",
    "                if wd != 0:\n",
    "                    p.data.add_(p.data, alpha=-lr * wd)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890190c-7b3a-418f-9a1f-eccbaadd19fb",
   "metadata": {},
   "source": [
    "## 3. 学习率调度（余弦退火 + warmup）\n",
    "\n",
    "按照 handout:\n",
    "\n",
    "* t < T_w: 线性 warmup 到 α_max\n",
    "* T_w ≤ t ≤ T_c: 余弦下降到 α_min\n",
    "* t > T_c: 固定 α_min\n",
    "\n",
    "我们实现成一个函数 `cosine_lr_schedule(step)`，并在训练循环里每个 step 调用它更新 optimizer.param_groups[i][\"lr\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117a4f26-0172-4919-9431-b5e0f26f4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_lr_schedule(\n",
    "    t: int,\n",
    "    lr_max: float,\n",
    "    lr_min: float,\n",
    "    warmup_iters: int,\n",
    "    cosine_iters: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Return lr_t for iteration t.\n",
    "    warmup_iters: Tw\n",
    "    cosine_iters: Tc\n",
    "    \"\"\"\n",
    "    if t < warmup_iters:\n",
    "        # linear warmup 0 -> lr_max\n",
    "        return lr_max * (t / max(1, warmup_iters))\n",
    "\n",
    "    if t <= cosine_iters:\n",
    "        # cosine anneal from lr_max -> lr_min\n",
    "        progress = (t - warmup_iters) / max(1, (cosine_iters - warmup_iters))\n",
    "        # cosine from 0..pi\n",
    "        cosine_term = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        return lr_min + (lr_max - lr_min) * cosine_term\n",
    "\n",
    "    # post-anneal\n",
    "    return lr_min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c9d20-0545-4c99-bcb4-99f86a9a421a",
   "metadata": {},
   "source": [
    "## 4. 梯度裁剪 (Gradient Clipping)\n",
    "\n",
    "思路：\n",
    "\n",
    "1. 统计所有参数梯度的整体 L2 范数\n",
    "2. 如果超过 max_norm，则按比例缩放每个 p.grad\n",
    "3. 加上一个很小的 eps 防止除0\n",
    "\n",
    "注意：这个函数 **就地修改** grad，是作业要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee8ca516-4b25-481a-b04d-920f1491e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clip_gradients(params: Iterable[torch.nn.Parameter], max_norm: float, eps: float = 1e-6):\n",
    "    \"\"\"\n",
    "    In-place gradient clipping to max global L2 norm.\n",
    "    \"\"\"\n",
    "    # gather all grads into a single norm\n",
    "    total_norm_sq = 0.0\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            total_norm_sq += float(torch.sum(p.grad.data.to(torch.float32) ** 2))\n",
    "\n",
    "    total_norm = math.sqrt(total_norm_sq)\n",
    "\n",
    "    if total_norm > max_norm:\n",
    "        scale = max_norm / (total_norm + eps)\n",
    "        for p in params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.data.mul_(scale)\n",
    "    # return the unclipped norm for logging\n",
    "    return total_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628dd31-1179-4ba5-a26e-459662694a28",
   "metadata": {},
   "source": [
    "## 5. get_batch: 从整段 token 序列中随机采样 batch\n",
    "\n",
    "输入：\n",
    "\n",
    "* `data`: numpy array / memmap of uint16 (token IDs)\n",
    "* `batch_size`\n",
    "* `context_len` (m)\n",
    "* `device` (e.g. \"cuda\")\n",
    "\n",
    "输出：\n",
    "\n",
    "* inputs: shape (B, m)\n",
    "* targets: shape (B, m)\n",
    "  两个 tensor 都放到 device 上。\n",
    "\n",
    "采样方法：\n",
    "\n",
    "* 随机起点 i\n",
    "* x[i : i+m] → input\n",
    "* x[i+1 : i+m+1] → target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f552265f-96f7-49e7-8dc5-497dc0421a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(\n",
    "    data: np.ndarray,\n",
    "    batch_size: int,\n",
    "    context_len: int,\n",
    "    device: str = \"cuda\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Randomly slice subsequences of length context_len from data,\n",
    "    and return (inputs, targets), each (B, context_len) on device.\n",
    "    data is a 1-D array of token IDs, e.g. uint16.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    # choose random starting indices in [0, n - context_len - 1]\n",
    "    # so that i+context_len is valid and i+context_len < n\n",
    "    starts = np.random.randint(0, n - context_len - 1, size=(batch_size,))\n",
    "    # build batches\n",
    "    x_batch = np.stack([data[i : i + context_len] for i in starts])           # (B, m)\n",
    "    y_batch = np.stack([data[i + 1 : i + 1 + context_len] for i in starts])   # (B, m)\n",
    "\n",
    "    # convert to torch\n",
    "    x_t = torch.tensor(x_batch, dtype=torch.long, device=device)\n",
    "    y_t = torch.tensor(y_batch, dtype=torch.long, device=device)\n",
    "    return x_t, y_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7836a-29d6-4908-bf2b-4f196af304a3",
   "metadata": {},
   "source": [
    "## 6. checkpoint 保存 / 读取\n",
    "\n",
    "我们要能：\n",
    "\n",
    "* 保存模型权重、优化器状态、当前 step/iter\n",
    "* 以后可以从 checkpoint 恢复训练\n",
    "\n",
    "用到：\n",
    "\n",
    "* `model.state_dict()`\n",
    "* `optimizer.state_dict()`\n",
    "* `torch.save / torch.load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2edf006e-a6a3-4b2e-96da-96fcc3fad163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, iteration: int, out_path: str):\n",
    "    \"\"\"\n",
    "    Save model/optimizer/iteration to a single file.\n",
    "    out_path: str or Path-like\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"iteration\": iteration,\n",
    "    }\n",
    "    torch.save(payload, out_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, src_path: str) -> int:\n",
    "    \"\"\"\n",
    "    Load from checkpoint file.\n",
    "    Returns the iteration we loaded.\n",
    "    \"\"\"\n",
    "    payload = torch.load(src_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(payload[\"model_state\"])\n",
    "    optimizer.load_state_dict(payload[\"optimizer_state\"])\n",
    "    iteration = payload[\"iteration\"]\n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328957d5-fd3f-4305-86a8-d4334600d148",
   "metadata": {},
   "source": [
    "## 7. TrainingConfig\n",
    "\n",
    "这是一个小的 dataclass，用来把训练 loop 里会用到的超参数集中管理，方便后面的主循环 notebook 引用。\n",
    "\n",
    "包含：\n",
    "\n",
    "* 模型结构\n",
    "* 优化器超参\n",
    "* scheduler 超参\n",
    "* 日志/保存相关配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c880df-1e33-45fa-bb82-7dbc1c803c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # data / batching\n",
    "    batch_size: int = 32\n",
    "    context_len: int = 256\n",
    "\n",
    "    # training steps\n",
    "    total_steps: int = 10_000\n",
    "    log_every: int = 50\n",
    "    eval_every: int = 500\n",
    "    ckpt_every: int = 1000\n",
    "\n",
    "    # optimizer (AdamW)\n",
    "    lr_max: float = 3e-4       # peak lr\n",
    "    lr_min: float = 3e-5       # final lr after cosine\n",
    "    betas: Tuple[float, float] = (0.9, 0.95)\n",
    "    eps: float = 1e-8\n",
    "    weight_decay: float = 0.1\n",
    "    grad_clip_norm: float = 1.0\n",
    "\n",
    "    # scheduler\n",
    "    warmup_iters: int = 200\n",
    "    cosine_iters: int = 10_000  # after this, lr = lr_min\n",
    "\n",
    "    # misc\n",
    "    device: str = \"cuda\"\n",
    "    mixed_precision: bool = True\n",
    "    ckpt_dir: str = \"./checkpoints\"\n",
    "    run_name: str = \"cs336_lm_run\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CS336)",
   "language": "python",
   "name": "cs336"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
