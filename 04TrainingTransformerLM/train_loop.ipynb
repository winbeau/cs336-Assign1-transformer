{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1568de42-341c-4a46-b8fe-74afc67c54b4",
   "metadata": {},
   "source": [
    "# CS336 Assignment 1 — Training Loop\n",
    "\n",
    "本 notebook 用 `training_functions.ipynb` 里实现的组件，加上你之前写好的 TransformerLM 模型，跑一个完整训练循环。\n",
    "\n",
    "主要功能：\n",
    "\n",
    "* memmap 加载 tokenized 数据\n",
    "* 采样 batch\n",
    "* 混合精度前向 + 反向\n",
    "* AdamW 更新\n",
    "* 余弦学习率调度\n",
    "* 梯度裁剪\n",
    "* 训练/验证 loss 日志\n",
    "* Matplotlib 实时曲线可视化（loss 下降情况）\n",
    "* checkpoint 保存\n",
    "* （可选）wandb 日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e1428c-2e9c-49c5-bfb9-75b50c618f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === bring in what we wrote in training_functions.ipynb ===\n",
    "# You can \"import\" them if you made a .py file,\n",
    "# or if you're running in one notebook environment, just run that notebook first and reuse.\n",
    "# from training_functions import (\n",
    "#     cross_entropy_loss,\n",
    "#     AdamW,\n",
    "#     cosine_lr_schedule,\n",
    "#     clip_gradients,\n",
    "#     get_batch,\n",
    "#     save_checkpoint,\n",
    "#     load_checkpoint,\n",
    "#     TrainingConfig,\n",
    "# )\n",
    "\n",
    "# Your Transformer LM\n",
    "# from transformerLM import TransformerLM  # <-- TODO: adjust to your actual module name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40daff-fcfd-4139-a647-7e2bb4dc6886",
   "metadata": {},
   "source": [
    "## 0. 准备数据 (memmap + sanity check)\n",
    "\n",
    "作业里建议把整个数据集（比如 TinyStories tokenized）保存成一个 `uint16` 的 `.npy`\n",
    "然后用 `np.memmap` 或 `np.load(..., mmap_mode='r')` 读取，避免一次性吃满内存。\n",
    "\n",
    "我们会准备：\n",
    "\n",
    "* `train_data`\n",
    "* `val_data`\n",
    "\n",
    "注意：`vocab_size` 应该和模型一致。\n",
    "`context_len` 要 ≤ 你模型声明的 `context_length`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0edf4471-2157-4790-abd1-e1fc0671f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token_dataset(path: str) -> np.memmap:\n",
    "    \"\"\"\n",
    "    path: .npy file of token IDs, dtype=uint16 (or int32 etc.)\n",
    "    We'll open in read-only memmap mode.\n",
    "    \"\"\"\n",
    "    arr = np.load(path, mmap_mode=\"r\")\n",
    "    # 可选 sanity check: vocab 范围\n",
    "    # print(\"max token id:\", int(arr.max()))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55757f5f-92c2-44ad-b505-faa15f2b33c5",
   "metadata": {},
   "source": [
    "## 1. 评估函数 eval_loss(model, data, config, num_batches)\n",
    "\n",
    "我们不想每次 eval 都跑全量数据，所以：\n",
    "\n",
    "* 随机抽 `num_batches`\n",
    "* 计算平均 loss (无梯度、eval 模式)\n",
    "* 返回这个标量，方便 logging / 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23f0ba2-0ea0-4c46-8e1b-d0a7ad678a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_loss(\n",
    "    model: nn.Module,\n",
    "    data: np.ndarray,\n",
    "    config: TrainingConfig,\n",
    "    num_batches: int = 20,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    device = config.device\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        x, y = get_batch(\n",
    "            data,\n",
    "            batch_size=config.batch_size,\n",
    "            context_len=config.context_len,\n",
    "            device=device,\n",
    "        )\n",
    "        # forward\n",
    "        logits = model(x)  # (B, T, vocab)\n",
    "        # shift: predict next token => targets already aligned in y\n",
    "        # we just flatten both B,T into one big batch dimension for CE\n",
    "        B, T, V = logits.shape\n",
    "        loss = cross_entropy_loss(\n",
    "            logits.view(B * T, V),\n",
    "            y.view(B * T),\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    model.train()\n",
    "    return float(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24cd24-f87a-4013-a92c-504fdba273d8",
   "metadata": {},
   "source": [
    "## 2. plotter: 实时更新训练/验证 loss 曲线\n",
    "\n",
    "我们用一个小的 helper 来维护 lists，然后每隔几步刷新一张图。\n",
    "注意：Jupyter 里用 `%matplotlib inline` 默认静态。如果你想动态刷新，可以 `clear_output(wait=True)`。\n",
    "下面实现一个简单的在线曲线记录类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2368a7-3e3f-46ca-9e5f-66d2c15028b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class LivePlotter:\n",
    "    def __init__(self):\n",
    "        self.train_steps = []\n",
    "        self.train_losses = []\n",
    "        self.eval_steps = []\n",
    "        self.eval_losses = []\n",
    "\n",
    "    def log_train(self, step, loss):\n",
    "        self.train_steps.append(step)\n",
    "        self.train_losses.append(loss)\n",
    "\n",
    "    def log_eval(self, step, loss):\n",
    "        self.eval_steps.append(step)\n",
    "        self.eval_losses.append(loss)\n",
    "\n",
    "    def draw(self):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        if self.train_steps:\n",
    "            plt.plot(self.train_steps, self.train_losses, label=\"train_loss\")\n",
    "        if self.eval_steps:\n",
    "            plt.plot(self.eval_steps, self.eval_losses, label=\"val_loss\")\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training Progress\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3cd71-f0b1-49f4-98f8-f463729e1c3b",
   "metadata": {},
   "source": [
    "## 3. create_model_and_optimizer(config, vocab_size, context_length, num_layers, d_model, num_heads)\n",
    "\n",
    "* 构建 TransformerLM\n",
    "* 构建 AdamW\n",
    "* GradScaler (混合精度)\n",
    "* 返回所有句柄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da6f6212-04f2-4e02-aeb9-49e381a992e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_optimizer(\n",
    "    config: TrainingConfig,\n",
    "    vocab_size: int,\n",
    "    context_length: int,\n",
    "    num_layers: int,\n",
    "    d_model: int,\n",
    "    num_heads: int,\n",
    "    d_ff: Optional[int] = None,  # if your model needs explicit ff dim\n",
    "    ckpt_path_to_resume: Optional[str] = None,\n",
    "):\n",
    "    device = config.device\n",
    "\n",
    "    # 1. 模型\n",
    "    model = TransformerLM(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        device=device,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # 2. 优化器\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.lr_max,  # we'll override per step using scheduler anyway\n",
    "        betas=config.betas,\n",
    "        eps=config.eps,\n",
    "        weight_decay=config.weight_decay,\n",
    "    )\n",
    "\n",
    "    # 3. GradScaler for AMP\n",
    "    scaler = GradScaler(enabled=config.mixed_precision)\n",
    "\n",
    "    # 4. maybe resume from checkpoint\n",
    "    start_step = 0\n",
    "    if ckpt_path_to_resume is not None and os.path.exists(ckpt_path_to_resume):\n",
    "        print(f\"[resume] Loading checkpoint from {ckpt_path_to_resume}\")\n",
    "        start_step = load_checkpoint(model, optimizer, ckpt_path_to_resume)\n",
    "        # Note: GradScaler state isn't in our checkpoint. You could store it too if you want.\n",
    "\n",
    "    return model, optimizer, scaler, start_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82eda07-144e-4e10-863a-529bb6f53788",
   "metadata": {},
   "source": [
    "## 4. train_loop(...)\n",
    "\n",
    "核心训练循环。\n",
    "每个 step：\n",
    "\n",
    "1. 抽 batch\n",
    "2. 计算当前 step 的学习率并塞回 optimizer\n",
    "3. 前向 (AMP autocast)\n",
    "4. loss 计算 (我们用自己写的 cross_entropy_loss)\n",
    "5. backward (scaled if AMP)\n",
    "6. gradient clipping\n",
    "7. optimizer.step(), zero_grad()\n",
    "\n",
    "另外：\n",
    "\n",
    "* 每隔 `log_every` 记录 train loss\n",
    "* 每隔 `eval_every` 在 val 上跑 evaluate_model_loss\n",
    "* 每隔 `ckpt_every` 保存 checkpoint\n",
    "* 用 LivePlotter 实时画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35aec45d-03c1-49d3-8a18-29b090ed832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "    train_data: np.ndarray,\n",
    "    val_data: np.ndarray,\n",
    "    config: TrainingConfig,\n",
    "    vocab_size: int,\n",
    "    context_length: int,\n",
    "    num_layers: int,\n",
    "    d_model: int,\n",
    "    num_heads: int,\n",
    "    d_ff: Optional[int] = None,\n",
    "    resume_ckpt: Optional[str] = None,\n",
    "    save_prefix: str = \"checkpoint_step\",\n",
    "    use_wandb: bool = False,\n",
    "):\n",
    "    device = config.device\n",
    "    os.makedirs(config.ckpt_dir, exist_ok=True)\n",
    "\n",
    "    # 可选 wandb\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        wandb.init(project=\"cs336_assignment1\", name=config.run_name, config=dict(\n",
    "            batch_size=config.batch_size,\n",
    "            context_len=config.context_len,\n",
    "            lr_max=config.lr_max,\n",
    "            lr_min=config.lr_min,\n",
    "            warmup_iters=config.warmup_iters,\n",
    "            cosine_iters=config.cosine_iters,\n",
    "            weight_decay=config.weight_decay,\n",
    "            grad_clip_norm=config.grad_clip_norm,\n",
    "            mixed_precision=config.mixed_precision,\n",
    "            model=dict(\n",
    "                vocab_size=vocab_size,\n",
    "                context_length=context_length,\n",
    "                num_layers=num_layers,\n",
    "                d_model=d_model,\n",
    "                num_heads=num_heads,\n",
    "                d_ff=d_ff,\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    # 初始化模型/优化器/缩放器\n",
    "    model, optimizer, scaler, step = create_model_and_optimizer(\n",
    "        config,\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=context_length,\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        ckpt_path_to_resume=resume_ckpt,\n",
    "    )\n",
    "\n",
    "    plotter = LivePlotter()\n",
    "\n",
    "    pbar = tqdm(range(step, config.total_steps), initial=step, total=config.total_steps)\n",
    "    for cur_step in pbar:\n",
    "        # === 1. 取一批数据 ===\n",
    "        x, y = get_batch(\n",
    "            train_data,\n",
    "            batch_size=config.batch_size,\n",
    "            context_len=config.context_len,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # === 2. 学习率调度，更新 optimizer group lr ===\n",
    "        lr_t = cosine_lr_schedule(\n",
    "            t=cur_step,\n",
    "            lr_max=config.lr_max,\n",
    "            lr_min=config.lr_min,\n",
    "            warmup_iters=config.warmup_iters,\n",
    "            cosine_iters=config.cosine_iters,\n",
    "        )\n",
    "        for g in optimizer.param_groups:\n",
    "            g[\"lr\"] = lr_t\n",
    "\n",
    "        # === 3. 前向 + loss ===\n",
    "        with autocast(enabled=config.mixed_precision, device_type=\"cuda\"):\n",
    "            logits = model(x)  # (B, T, vocab)\n",
    "            B, T, V = logits.shape\n",
    "            loss = cross_entropy_loss(\n",
    "                logits.view(B*T, V),\n",
    "                y.view(B*T),\n",
    "            )\n",
    "\n",
    "        # === 4. 反向传播 ===\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # === 5. 梯度裁剪 ===\n",
    "        # 先 unscale 再裁剪 (amp best practice)\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = clip_gradients(model.parameters(), config.grad_clip_norm)\n",
    "\n",
    "        # === 6. Optimizer Step (AMP aware) ===\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # === logging ===\n",
    "        if (cur_step % config.log_every) == 0:\n",
    "            train_loss_value = float(loss.item())\n",
    "            plotter.log_train(cur_step, train_loss_value)\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"step {cur_step} \"\n",
    "                f\"loss {train_loss_value:.4f} \"\n",
    "                f\"lr {lr_t:.2e} \"\n",
    "                f\"gnorm {grad_norm:.2f}\"\n",
    "            )\n",
    "\n",
    "            if use_wandb:\n",
    "                import wandb\n",
    "                wandb.log({\n",
    "                    \"train/loss\": train_loss_value,\n",
    "                    \"train/lr\": lr_t,\n",
    "                    \"train/grad_norm\": grad_norm,\n",
    "                    \"step\": cur_step,\n",
    "                })\n",
    "\n",
    "        # === eval ===\n",
    "        if (cur_step % config.eval_every) == 0 and cur_step > 0:\n",
    "            val_loss_value = evaluate_model_loss(model, val_data, config)\n",
    "            plotter.log_eval(cur_step, val_loss_value)\n",
    "\n",
    "            if use_wandb:\n",
    "                import wandb\n",
    "                wandb.log({\n",
    "                    \"val/loss\": val_loss_value,\n",
    "                    \"step\": cur_step,\n",
    "                })\n",
    "\n",
    "        # === 可视化刷新 ===\n",
    "        if (cur_step % config.log_every) == 0:\n",
    "            plotter.draw()\n",
    "\n",
    "        # === checkpoint ===\n",
    "        if (cur_step % config.ckpt_every) == 0 and cur_step > 0:\n",
    "            ckpt_path = os.path.join(\n",
    "                config.ckpt_dir, f\"{save_prefix}_{cur_step}.pt\"\n",
    "            )\n",
    "            save_checkpoint(model, optimizer, cur_step, ckpt_path)\n",
    "            # 你也可以顺带保存 plotter 的数据等\n",
    "\n",
    "    # 训练结束，最后再存一个最终 checkpoint\n",
    "    final_ckpt = os.path.join(config.ckpt_dir, f\"{save_prefix}_final.pt\")\n",
    "    save_checkpoint(model, optimizer, config.total_steps, final_ckpt)\n",
    "\n",
    "    # 最后一张完整曲线\n",
    "    plotter.draw()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab25427-3c67-40fa-9fae-6a9f4182733c",
   "metadata": {},
   "source": [
    "## 5. 使用示例\n",
    "\n",
    "下面是一个最小可跑示例（debug规模），假设：\n",
    "\n",
    "* 你有两个 `.npy`：`tinystories_train_tokens.npy` 和 `tinystories_val_tokens.npy`\n",
    "* 这些是 uint16 token ids\n",
    "* 你的 `TransformerLM` 支持这些超参\n",
    "\n",
    "注意：第一次调试可以把 `total_steps`、`context_len`、`batch_size` 都调小，确保 loop 能跑通、loss 会下降。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe0b13-33d5-4ea1-9dde-5edb10012107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 载入 memmap 数据（TinyStories 的tokens）\n",
    "train_data = load_token_dataset(\"../datasets/TinyStories/tokens_train.npy\")\n",
    "val_data   = load_token_dataset(\"../datasets/TinyStories/tokens_valid.npy)\n",
    "\n",
    "    # 2. 配置\n",
    "cfg = TrainingConfig(\n",
    "    batch_size=32,\n",
    "    context_len=128,\n",
    "    total_steps=2000,\n",
    "    log_every=20,\n",
    "    eval_every=200,\n",
    "    ckpt_every=500,\n",
    "    lr_max=3e-4,\n",
    "    lr_min=3e-5,\n",
    "    warmup_iters=100,\n",
    "    cosine_iters=2000,\n",
    "    weight_decay=0.1,\n",
    "    grad_clip_norm=1.0,\n",
    "    device=\"cuda\",\n",
    "    mixed_precision=True,\n",
    "    ckpt_dir=\"./checkpoints_tinystories\",\n",
    "    run_name=\"tinystories_debug_run\",\n",
    ")\n",
    "\n",
    "    # 3. 模型超参（示例：小模型以便CPU/GPU都能跑）\n",
    "vocab_size     = 10_000      # e.g. tokenizer vocab size\n",
    "context_length = cfg.context_len\n",
    "num_layers     = 4\n",
    "d_model        = 256\n",
    "num_heads      = 4\n",
    "d_ff           = int((8/3) * d_model)  # or whatever your impl expects\n",
    "\n",
    "trained_model = train_loop(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    config=cfg,\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=context_length,\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    resume_ckpt=None,            # or \"./checkpoints_tinystories/checkpoint_step_1000.pt\"\n",
    "    save_prefix=\"tinystories_lm\",\n",
    "    use_wandb=False,             # turn on if you want wandb logging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ae42b-2df0-46ac-8c76-8a7748ee8c49",
   "metadata": {},
   "source": [
    "## 检查数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94af446a-ce4c-48a1-a46e-462dc3930422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid example: [ 83 112 111 116  46  32  83 112 111 116  32 115  97 119  32 116 104 101\n",
      "  32 115 104 105 110 121  32  99  97 114  32  97 110 100  32 115  97 105\n",
      " 100  44  32  34  87 111 119  44  32  75 105 116 116 121  44  32 121 111\n",
      " 117 114  32  99  97 114  32 105 115  32 115 111  32  98 114 105 103 104\n",
      " 116  32  97 110 100  32  99 108 101  97 110  32  75 105 116 116 121  32\n",
      " 115 109 105 108 101 100  32  97 110 100]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# train_data = np.load(\"../datasets/TinyStories/tokens_train.npy\", mmap_mode=\"r\")\n",
    "valid_data = np.load(\"../datasets/TinyStories/tokens_valid.npy\", mmap_mode=\"r\")\n",
    "# print(train_data.dtype, train_data.shape)\n",
    "print(\"valid example:\", valid_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc3c4c-18b5-4b4f-86ae-f0072601caa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CS336)",
   "language": "python",
   "name": "cs336"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
